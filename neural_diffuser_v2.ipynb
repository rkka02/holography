{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d514908",
   "metadata": {},
   "source": [
    "<h1> Neural Net </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97e40d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from lucam import Lucam\n",
    "\n",
    "# --- 기본 설정 (이전과 동일) ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "N = 600\n",
    "total_cam_roi = (450, -500, 450, -500) # Top Bottom Left Right\n",
    "first_order_cam_roi = (500, -450, 450, -500) # Top Bottom Left Right\n",
    "\n",
    "LEARNING_RATE_MODEL = 1e-3\n",
    "LEARNING_RATE_PHASE = 1e-2\n",
    "\n",
    "# --- 🧠 중간 깊이의 뉴럴 네트워크 모델 정의 ---\n",
    "class MediumUNetPropagation(nn.Module):\n",
    "    def __init__(self, in_channels=2, out_channels=1):\n",
    "        super(MediumUNetPropagation, self).__init__()\n",
    "\n",
    "        # --- 인코더 (Contracting Path) ---\n",
    "        # Level 1\n",
    "        self.enc1 = self.conv_block(in_channels, 64)\n",
    "        # Level 2\n",
    "        self.enc2 = self.conv_block(64, 128)\n",
    "        # Level 3 (추가된 깊이)\n",
    "        self.enc3 = self.conv_block(128, 256)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # --- 병목 구간 (Bottleneck) ---\n",
    "        self.bottleneck = self.conv_block(256, 512)\n",
    "\n",
    "        # --- 디코더 (Expanding Path) ---\n",
    "        # Level 3\n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.dec3 = self.conv_block(256 + 256, 256) # Skip connection 포함\n",
    "        # Level 2\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.dec2 = self.conv_block(128 + 128, 128) # Skip connection 포함\n",
    "        # Level 1\n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dec1 = self.conv_block(64 + 64, 64)   # Skip connection 포함\n",
    "\n",
    "        # --- 최종 출력 레이어 ---\n",
    "        self.out_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "    def conv_block(self, in_c, out_c):\n",
    "        \"\"\"두 개의 3x3 Conv와 ReLU, BatchNorm으로 구성된 기본 블록\"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_c, out_c, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_c, out_c, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, phase_map):\n",
    "        # ‼️ 입력 변환: φ -> [cos(φ), sin(φ)]\n",
    "        if phase_map.dim() == 3: # (B, H, W) -> (B, 1, H, W)\n",
    "            phase_map = phase_map.unsqueeze(1)\n",
    "\n",
    "        x_cos = torch.cos(phase_map)\n",
    "        x_sin = torch.sin(phase_map)\n",
    "        x = torch.cat([x_cos, x_sin], dim=1) # (B, 2, N, N)\n",
    "\n",
    "        # --- 인코더 ---\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool(e1))\n",
    "        e3 = self.enc3(self.pool(e2))\n",
    "\n",
    "        # --- 병목 ---\n",
    "        b = self.bottleneck(self.pool(e3))\n",
    "\n",
    "        # --- 디코더 + Skip Connections ---\n",
    "        d3 = self.upconv3(b)\n",
    "        d3 = torch.cat([d3, e3], dim=1)\n",
    "        d3 = self.dec3(d3)\n",
    "\n",
    "        d2 = self.upconv2(d3)\n",
    "        d2 = torch.cat([d2, e2], dim=1)\n",
    "        d2 = self.dec2(d2)\n",
    "\n",
    "        d1 = self.upconv1(d2)\n",
    "        d1 = torch.cat([d1, e1], dim=1)\n",
    "        d1 = self.dec1(d1)\n",
    "\n",
    "        # --- 출력 ---\n",
    "        out = self.out_conv(d1)\n",
    "        return out.squeeze(1) # (B, N, N)\n",
    "\n",
    "# --- 헬퍼 함수 정의 (이전과 동일) ---\n",
    "def save_phase_as_image(phase_tensor, filename):\n",
    "    phase_normalized = (phase_tensor.detach() + torch.pi) / (2 * torch.pi)\n",
    "    phase_uint8 = (phase_normalized * 255).byte().cpu().numpy()\n",
    "    Image.fromarray(phase_uint8).save(filename)\n",
    "\n",
    "def load_and_preprocess_image(path, size=(N, N)):\n",
    "    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None: raise FileNotFoundError(f\"'{path}' 파일을 찾을 수 없습니다.\")\n",
    "    img = cv2.resize(img, dsize=size)\n",
    "    img_float = img.astype(np.float32) / np.max(img) # 0~1 사이로 정규화\n",
    "    return torch.from_numpy(img_float).to('cpu')\n",
    "\n",
    "# --- 🌟 데이터셋 클래스 정의 🌟 ---\n",
    "class HolographyDataset(Dataset):\n",
    "    def __init__(self, image_dir):\n",
    "        # 이미지 파일 경로 리스트 가져오기\n",
    "        self.image_paths = glob.glob(os.path.join(image_dir, '*.jpg')) + \\\n",
    "                           glob.glob(os.path.join(image_dir, '*.png'))\n",
    "                           \n",
    "        lam = 0.532e-6\n",
    "        dx = 12.5e-6\n",
    "        z = 100e-3\n",
    "\n",
    "        lam = torch.tensor(lam).cuda()\n",
    "        dx = torch.tensor(dx).cuda()\n",
    "        z = torch.tensor(z).cuda()\n",
    "\n",
    "        # 각 이미지에 대한 위상 텐서를 저장할 딕셔너리\n",
    "        self.phase_tensors = {}\n",
    "        for path in self.image_paths:\n",
    "            # 초기 위상은 랜덤으로 생성\n",
    "            phase = (torch.pi * torch.ones(N,N)).requires_grad_(True)\n",
    "            self.phase_tensors[path] = phase\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path = self.image_paths[idx]\n",
    "        target_intensity = load_and_preprocess_image(path)\n",
    "        target_intensity = target_intensity / torch.max(target_intensity).item()\n",
    "        target_amplitude = torch.sqrt(target_intensity)\n",
    "        phase_tensor = self.phase_tensors[path]\n",
    "        return target_amplitude.to('cuda'), phase_tensor.to('cuda'), path # 경로도 함께 반환하여 추적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38ed97a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(44, 100):\n",
    "    pattern = np.random.rand(600, 600) * 255\n",
    "    cv2.imwrite('test.png', pattern)\n",
    "\n",
    "    slm_process = subprocess.Popen(['python', 'test.py'])\n",
    "    time.sleep(2)\n",
    "    \n",
    "    camera = Lucam()\n",
    "    capture = camera.TakeSnapshot()\n",
    "    capture = cv2.resize(capture, dsize=(600, 600))\n",
    "\n",
    "    slm_process.terminate()\n",
    "    slm_process.wait()\n",
    "    \n",
    "    cv2.imwrite(f'dataset/trainset/patterns/{i}.png', pattern)\n",
    "    cv2.imwrite(f'dataset/trainset/captures/{i}.png', capture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "996f2ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 🌟 데이터셋 클래스 정의 🌟 ---\n",
    "class SpeckleDataset(Dataset):\n",
    "    def __init__(self, pattern_dir, capture_dir):\n",
    "        # 이미지 파일 경로 리스트 가져오기\n",
    "        self.pattern_paths = sorted(glob.glob(os.path.join(pattern_dir, '*.png')))\n",
    "        self.target_paths = sorted(glob.glob(os.path.join(capture_dir, '*.png')))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pattern_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pattern_path = self.pattern_paths[idx]\n",
    "        # '패턴 이미지'는 0-255 grayscale로 저장된 위상 정보를 나타낸다고 가정\n",
    "        pattern_img = cv2.imread(pattern_path, cv2.IMREAD_GRAYSCALE)\n",
    "        pattern_img = cv2.resize(pattern_img, dsize=(N, N))\n",
    "        \n",
    "        # 0~255 값을 0~2π 범위의 위상(phase)으로 변환\n",
    "        phase_map = (pattern_img.astype(np.float32) / 255.0) * 2 * np.pi\n",
    "        phase_map_tensor = torch.from_numpy(phase_map)\n",
    "\n",
    "        target_path = self.target_paths[idx]\n",
    "        target_intensity = load_and_preprocess_image(target_path)\n",
    "        target_intensity = target_intensity / torch.max(target_intensity).item()\n",
    "        target_amplitude = torch.sqrt(target_intensity)\n",
    "\n",
    "        return phase_map_tensor.to('cuda'), target_amplitude.to('cuda')\n",
    "\n",
    "# --- 변수 및 모델 초기화 ---\n",
    "model = MediumUNetPropagation().to(device)\n",
    "\n",
    "# 🌟 데이터셋 및 데이터로더 생성\n",
    "# 'images' 폴더에 학습용 이미지를 넣어주세요.\n",
    "train_dataset = SpeckleDataset(pattern_dir='./dataset/train/patterns',\n",
    "                               capture_dir='./dataset/train/captures')\n",
    "test_dataset = SpeckleDataset(pattern_dir='./dataset/test/patterns',\n",
    "                               capture_dir='./dataset/test/captures')\n",
    "\n",
    "# 미니배치 크기. GPU 메모리에 따라 조절.\n",
    "BATCH_SIZE = 1\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# s1, s2 스케일 팩터. 이제 이미지마다 필요할 수 있으나, 우선은 공유\n",
    "s = torch.tensor(1.0, device=device, requires_grad=True)\n",
    "\n",
    "# ‼️ 옵티마이저 정의. 이제 위상 텐서는 데이터셋 안에 있으므로, 별도로 최적화\n",
    "optimizer_model = optim.Adam(list(model.parameters()) + [s], lr=LEARNING_RATE_MODEL)\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4673bae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Epoch 1/20 ====================\n",
      "Epoch 1, Batch 1 모델 업데이트 완료. Train Loss : 0.245718\n",
      "Epoch 1, Batch 11 모델 업데이트 완료. Train Loss : 0.051116\n",
      "Epoch 1, Batch 21 모델 업데이트 완료. Train Loss : 0.040201\n",
      "Epoch 1, Batch 31 모델 업데이트 완료. Train Loss : 0.034389\n",
      "Epoch 1, Batch 41 모델 업데이트 완료. Train Loss : 0.032959\n",
      "Epoch 1, Batch 51 모델 업데이트 완료. Train Loss : 0.032424\n",
      "Epoch 1, Batch 61 모델 업데이트 완료. Train Loss : 0.031423\n",
      "Epoch 1, Batch 71 모델 업데이트 완료. Train Loss : 0.031982\n",
      "\n",
      "==================== Epoch 2/20 ====================\n",
      "Epoch 2, Batch 1 모델 업데이트 완료. Train Loss : 0.031591\n",
      "Epoch 2, Batch 11 모델 업데이트 완료. Train Loss : 0.031814\n",
      "Epoch 2, Batch 21 모델 업데이트 완료. Train Loss : 0.031957\n",
      "Epoch 2, Batch 31 모델 업데이트 완료. Train Loss : 0.031799\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# data_loader에서 미니배치 단위로 데이터를 가져옴\u001b[39;00m\n\u001b[32m     11\u001b[39m model = model.train()\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern_amplitudes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_amplitudes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     13\u001b[39m \n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# --- 단계 1: 위상 업데이트 -\u001b[39;49;00m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# U-Net 모델은 배치 입력을 처리할 수 있도록 수정됨\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\holo\\envs\\holo\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\holo\\envs\\holo\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\holo\\envs\\holo\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mSpeckleDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     19\u001b[39m phase_map_tensor = torch.from_numpy(phase_map)\n\u001b[32m     21\u001b[39m target_path = \u001b[38;5;28mself\u001b[39m.target_paths[idx]\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m target_intensity = \u001b[43mload_and_preprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m target_intensity = target_intensity / torch.max(target_intensity).item()\n\u001b[32m     24\u001b[39m target_amplitude = torch.sqrt(target_intensity)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 109\u001b[39m, in \u001b[36mload_and_preprocess_image\u001b[39m\u001b[34m(path, size)\u001b[39m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_and_preprocess_image\u001b[39m(path, size=(N, N)):\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m     img = \u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mIMREAD_GRAYSCALE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m img \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m 파일을 찾을 수 없습니다.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    111\u001b[39m     img = cv2.resize(img, dsize=size)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from pytorch_msssim import ssim, ms_ssim # Multi-Scale SSIM이 더 성능이 좋을 수 있습니다.\n",
    "import torch.nn.functional as F\n",
    "\n",
    "NUM_EPOCHS = 2 # 전체 데이터셋을 몇 번 반복할지\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\n{'='*20} Epoch {epoch + 1}/{NUM_EPOCHS} {'='*20}\")\n",
    "    \n",
    "    # data_loader에서 미니배치 단위로 데이터를 가져옴\n",
    "    model = model.train()\n",
    "    for i, (pattern_amplitudes, target_amplitudes) in enumerate(train_loader):\n",
    "        \n",
    "        # --- 단계 1: 위상 업데이트 -\n",
    "        optimizer_model.zero_grad()\n",
    "        \n",
    "        # U-Net 모델은 배치 입력을 처리할 수 있도록 수정됨\n",
    "        prediction = model(pattern_amplitudes**2)\n",
    "\n",
    "        loss_train = loss_fn(s * prediction, target_amplitudes**2)\n",
    "        loss_train.backward()\n",
    "        optimizer_model.step()\n",
    "        \n",
    "        if i%10==0:\n",
    "            print(f\"Epoch {epoch+1}, Batch {i+1} 모델 업데이트 완료. Train Loss : {loss_train.item():.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98ab18ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 Test Loss : 0.031475\n",
      "Batch 2 Test Loss : 0.031648\n",
      "Batch 3 Test Loss : 0.031562\n",
      "Batch 4 Test Loss : 0.031512\n",
      "Batch 5 Test Loss : 0.032123\n",
      "Batch 6 Test Loss : 0.031546\n",
      "Batch 7 Test Loss : 0.031795\n",
      "Batch 8 Test Loss : 0.031342\n",
      "Batch 9 Test Loss : 0.032039\n",
      "Batch 10 Test Loss : 0.032106\n",
      "Batch 11 Test Loss : 0.031734\n",
      "Batch 12 Test Loss : 0.031614\n",
      "Batch 13 Test Loss : 0.031771\n",
      "Batch 14 Test Loss : 0.031430\n",
      "Batch 15 Test Loss : 0.031634\n",
      "Batch 16 Test Loss : 0.031535\n",
      "Batch 17 Test Loss : 0.031200\n",
      "Batch 18 Test Loss : 0.032145\n",
      "Batch 19 Test Loss : 0.031499\n",
      "Batch 20 Test Loss : 0.031671\n"
     ]
    }
   ],
   "source": [
    "# data_loader에서 미니배치 단위로 데이터를 가져옴\n",
    "model = model.eval()\n",
    "with torch.no_grad():\n",
    "    losses = []\n",
    "    for i, (pattern_amplitudes, target_amplitudes) in enumerate(test_loader):\n",
    "            \n",
    "        # U-Net 모델은 배치 입력을 처리할 수 있도록 수정됨\n",
    "        prediction = model(pattern_amplitudes**2)\n",
    "\n",
    "        loss_test = loss_fn(s * prediction, target_amplitudes**2)\n",
    "\n",
    "        print(f\"Batch {i+1} Test Loss : {loss_test:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "91cc8660",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'speckle_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "812b9a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 변수 및 모델 초기화 ---\n",
    "model = MediumUNetPropagation().to(device)\n",
    "model.load_state_dict(torch.load('speckle_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e728169",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = Image.open('apple.png').convert('L')\n",
    "target = np.array(target)\n",
    "target = cv2.resize(target, dsize=(600,600))\n",
    "target = target / np.max(target)\n",
    "target = np.sqrt(target)\n",
    "target = torch.from_numpy(target).cuda()\n",
    "target = target.type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d68a0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase = torch.ones(600,600) * torch.pi\n",
    "phase = phase.type(torch.float32)\n",
    "phase = phase.requires_grad_()\n",
    "optimizer = torch.optim.Adam([phase], lr=1e-2)\n",
    "phase = phase.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4627a682",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\holo\\envs\\holo\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:610: UserWarning: Using a target size (torch.Size([600, 600])) that is different to the input size (torch.Size([1, 600, 600])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss : 0.367850\n",
      "Epoch 10, Loss : 0.367850\n",
      "Epoch 20, Loss : 0.367850\n",
      "Epoch 30, Loss : 0.367850\n",
      "Epoch 40, Loss : 0.367850\n",
      "Epoch 50, Loss : 0.367850\n",
      "Epoch 60, Loss : 0.367850\n",
      "Epoch 70, Loss : 0.367850\n",
      "Epoch 80, Loss : 0.367850\n",
      "Epoch 90, Loss : 0.367850\n"
     ]
    }
   ],
   "source": [
    "model = model.eval()\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    prediction = model(phase.unsqueeze(0))\n",
    "    loss = loss_fn(prediction, target**2)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i%10 == 0:\n",
    "        print(f'Epoch {i}, Loss : {loss.item():.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31c08e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase = phase.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25e58933",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase = phase / np.max(phase) * 255\n",
    "Image.fromarray(phase.astype('uint8')).save('test.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "holo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
