{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e69e88e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57506165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = np.zeros((600,600))\n",
    "cv2.imwrite('dataset/zeros.png', pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a50adf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "N = 224\n",
    "random_phase = np.random.rand(4*N, 4*N) * 2 * np.pi\n",
    "def create_speckle(phase, lam=0.532e-6, f1=0.20, f2=0.10):\n",
    "    N   = phase.shape[0]\n",
    "    Np  = 4 * N                    # 패딩된 격자 크기 # 4 * 224 = 896\n",
    "    pad = (Np - N)//2              # (896 - 224)//2 = 336\n",
    "    \n",
    "    # 1) 입력면\n",
    "    M = np.exp(1j * phase)\n",
    "    M = np.pad(M, pad_width=pad, mode='constant', constant_values=0)\n",
    "    \n",
    "    # 2) 푸리에면 무작위 위상 디퓨저\n",
    "    D = np.exp(1j * random_phase)          # 위상만 가진 디퓨저\n",
    "    \n",
    "    # 3) 1차 FFT (푸리에면)\n",
    "    F = np.fft.fftshift(np.fft.fft2(np.fft.fftshift(M))) * D\n",
    "    \n",
    "    # 4) 2차 FFT (영상면)\n",
    "    U = np.fft.ifftshift(np.fft.ifft2(np.fft.ifftshift(F)))\n",
    "    \n",
    "    # 배율 보정 & 뒤집기\n",
    "    U *= (1/(lam**2 * f1 * f2))\n",
    "    U = np.flip(U, axis=(0, 1))\n",
    "    \n",
    "    # 5) 결과 크롭 (원본보다 2배 넓게 보기)\n",
    "    out_size = 2*N           # 예: 2N×2N ROI # 2*224=448\n",
    "    start = (Np - out_size)//2\n",
    "    I = np.abs(U[start:start+out_size, start:start+out_size])**2\n",
    "    \n",
    "    # 필요하면 다시 0.5배 축소(실제 배율) 후 붙여넣기\n",
    "    I_small = cv2.resize(I, dsize=(N, N), interpolation=cv2.INTER_AREA)\n",
    "    I_small = I_small / np.max(I_small[N//4:3*N//4, N//4:3*N//4])\n",
    "    return I_small\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73cf687b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:08<00:00, 12.35it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "path = 'dataset/train/phase'\n",
    "vmax =  2\n",
    "for file in tqdm(os.listdir(path)):\n",
    "    phase = Image.open(os.path.join(path, file)).convert('L')\n",
    "    phase = np.array(phase)\n",
    "    phase = phase / 255 * 2 * np.pi\n",
    "    phase = cv2.resize(phase, dsize=(224, 224))\n",
    "    speckle = create_speckle(phase)\n",
    "    speckle = speckle / vmax * 225\n",
    "    Image.fromarray(speckle.astype('uint8')).save(f'dataset/train/speckle/{file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27d11d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1단계: 전체 데이터셋의 99.9 퍼센타일 값을 계산합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:08<00:00, 12.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "계산된 99.9 퍼센타일 (vmax): 1.9363751369183269\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "# create_speckle 함수가 정의되어 있다고 가정합니다.\n",
    "# 예시: def create_speckle(phase): ... return speckle_pattern\n",
    "\n",
    "# --- 경로 설정 ---\n",
    "path = 'dataset/train/phase'\n",
    "\n",
    "all_speckle_values = []\n",
    "\n",
    "print(\"1단계: 전체 데이터셋의 99.9 퍼센타일 값을 계산합니다...\")\n",
    "for file in tqdm(os.listdir(path)):\n",
    "    # 이미지를 불러오고 phase 맵으로 변환\n",
    "    phase = Image.open(os.path.join(path, file)).convert('L')\n",
    "    phase = np.array(phase)\n",
    "    phase = phase / 255.0 * 2 * np.pi\n",
    "    phase = cv2.resize(phase, dsize=(224, 224))\n",
    "\n",
    "    # 스페클 생성 (이 함수는 이미 정의되어 있어야 합니다)\n",
    "    speckle = create_speckle(phase)\n",
    "\n",
    "    # 생성된 스페클 값들을 하나의 리스트에 모두 추가\n",
    "    all_speckle_values.extend(speckle.flatten())\n",
    "\n",
    "# 모든 스페클 값들 중에서 99.9 퍼센타일 계산\n",
    "# 데이터셋이 매우 크면 이 부분에서 메모리를 많이 사용할 수 있습니다.\n",
    "vmax_p999 = np.percentile(all_speckle_values, 99.99)\n",
    "\n",
    "print(f\"\\n계산된 99.9 퍼센타일 (vmax): {vmax_p999}\")\n",
    "\n",
    "# 이제 이 vmax_p999 값을 2단계에서 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8664374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "def get_slm_grey_level(desired_phase, calibration_data):\n",
    "\n",
    "    if not calibration_data:\n",
    "        raise ValueError(\"SLM calibration data cannot be empty.\")\n",
    "    \n",
    "    # 그레이 레벨 배열 (0, 1, 2, ..., 255)\n",
    "    grey_levels = np.arange(len(calibration_data))\n",
    "\n",
    "    # interp1d 함수를 사용하여 보간 함수 생성\n",
    "    # x: 위상 값 (calibration_data), y: 그레이 레벨 (grey_levels)\n",
    "    # kind='linear': 선형 보간\n",
    "    # bounds_error=False: 입력 위상이 calibration_data 범위를 벗어나도 에러 발생 안 함\n",
    "    # fill_value=(grey_levels[0], grey_levels[-1]): 범위 밖의 값은 최소/최대 그레이 레벨로 클램프\n",
    "    interpolation_function = interp1d(\n",
    "        calibration_data,\n",
    "        grey_levels,\n",
    "        kind='linear',\n",
    "        bounds_error=False,\n",
    "        fill_value=(grey_levels[0], grey_levels[-1])\n",
    "    )\n",
    "\n",
    "    # 원하는 위상에 해당하는 그레이 레벨 계산\n",
    "    interpolated_grey_level = interpolation_function(desired_phase)\n",
    "\n",
    "    # 결과를 가장 가까운 정수로 반올림하고, 정수형으로 변환하여 반환\n",
    "    # .item()은 numpy 배열이 아닌 스칼라 값을 반환하도록 합니다.\n",
    "    return int(np.round(interpolated_grey_level).item())\n",
    "\n",
    "loaded_data = []\n",
    "with open('LUT.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        stripped_line = line.strip() # 공백(줄바꿈 포함) 제거\n",
    "        if stripped_line: # 빈 줄이 아닌 경우에만 처리\n",
    "            loaded_data.append(float(stripped_line))\n",
    "            \n",
    "grey_levels = np.arange(len(loaded_data))\n",
    "\n",
    "phase_to_grey_level_interpolator = interp1d(\n",
    "    loaded_data,\n",
    "    grey_levels,\n",
    "    kind='linear',\n",
    "    bounds_error=False,\n",
    "    fill_value=(grey_levels[0], grey_levels[-1])\n",
    ")\n",
    "\n",
    "grey_to_phase_level_interpolator = interp1d(\n",
    "    grey_levels,\n",
    "    loaded_data,\n",
    "    kind='linear',\n",
    "    bounds_error=False,\n",
    "    fill_value=(grey_levels[0], grey_levels[-1])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3d15bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# --- 기본 설정 (이전과 동일) ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "N = 600\n",
    "LEARNING_RATE_MODEL = 1e-4\n",
    "LEARNING_RATE_PHASE = 1e-3\n",
    "\n",
    "\n",
    "# --- 🌟 데이터셋 클래스 정의 🌟 ---\n",
    "class SpeckleDataset(Dataset):\n",
    "    def __init__(self, phase_dir, speckle_dir):\n",
    "        # 이미지 파일 경로 리스트 가져오기\n",
    "        self.phase_paths = sorted(os.listdir(phase_dir))\n",
    "        self.speckle_paths = sorted(os.listdir(speckle_dir))\n",
    "\n",
    "        self.phase_list = []\n",
    "        self.speckle_list = []\n",
    "\n",
    "        # 각 이미지에 대한 위상을 저장할 딕셔너리\n",
    "        for path in self.phase_paths:\n",
    "            phase = Image.open(os.path.join(phase_dir, path)).convert('L')\n",
    "            phase = np.array(phase)\n",
    "            \n",
    "            phase = cv2.resize(phase, dsize=(N, N))\n",
    "            phase = phase / np.max(phase) * 2 * np.pi\n",
    "            phase = np.clip(phase_to_grey_level_interpolator(phase), 0, 255).astype('uint8')\n",
    "            \n",
    "            phase = grey_to_phase_level_interpolator(phase)\n",
    "            # phase = np.pad(phase, pad_width=(2048-N)//2, mode='constant', constant_values=0)\n",
    "            self.phase_list.append(phase)\n",
    "        \n",
    "        for path in self.speckle_paths:\n",
    "            speckle = Image.open(os.path.join(speckle_dir, path)).convert('L')\n",
    "            speckle = np.array(speckle)\n",
    "            speckle = speckle / np.max(speckle)\n",
    "            # speckle = cv2.resize(speckle, dsize=(N, N))\n",
    "            self.speckle_list.append(speckle)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.phase_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        phase = self.phase_list[idx]\n",
    "        speckle = self.speckle_list[idx]\n",
    "        phase = torch.from_numpy(phase)\n",
    "        speckle = torch.from_numpy(speckle)\n",
    "        return phase, speckle\n",
    "    \n",
    "train_dataset = SpeckleDataset('dataset/diffuser/train/phase', 'dataset/diffuser/train/speckle')\n",
    "test_dataset = SpeckleDataset('dataset/diffuser/test/phase', 'dataset/diffuser/test/speckle')\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02656b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_msssim import ssim, ms_ssim # <--- 이 부분을 추가하세요\n",
    "import torch.nn.functional as F\n",
    "from skimage import feature\n",
    "\n",
    "# --- 기본 설정 (이전과 동일) ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "N = 600\n",
    "LEARNING_RATE_MODEL = 1e-4\n",
    "LEARNING_RATE_PHASE = 1e-3\n",
    "\n",
    "import torch, math\n",
    "import torch.nn as nn\n",
    "\n",
    "# ───────────────── ① 고정 pupil 생성 ─────────────────\n",
    "def make_pupil(N, radius_px=10, y_shift_px=-10):\n",
    "    x = torch.arange(-N//2, N//2)\n",
    "    X, Y = torch.meshgrid(x, x, indexing='ij')\n",
    "    P = (((X+y_shift_px)**2 + Y**2) < radius_px**2).float()\n",
    "    return P          # (N,N) real, 0/1\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# ─────────────────── Residual DoubleConv ───────────────────\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "        )\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        # 입력·출력 채널이 다르면 1×1 프로젝션\n",
    "        self.skip = nn.Identity() if in_ch == out_ch else nn.Conv2d(in_ch, out_ch, 1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.conv(x) + self.skip(x))\n",
    "\n",
    "# ─────────────────── Channel Attention (Squeeze & Excite) ───────────────────\n",
    "class SE(nn.Module):\n",
    "    def __init__(self, ch, r=8):\n",
    "        super().__init__()\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc   = nn.Sequential(nn.Linear(ch, ch//r, bias=False),\n",
    "                                  nn.ReLU(inplace=True),\n",
    "                                  nn.Linear(ch//r, ch, bias=False),\n",
    "                                  nn.Sigmoid())\n",
    "    def forward(self, x):\n",
    "        b,c,_,_ = x.shape\n",
    "        w = self.pool(x).view(b, c)\n",
    "        w = self.fc(w).view(b, c, 1, 1)\n",
    "        return x * w\n",
    "\n",
    "# ─────────────────── Deep U-Net ───────────────────\n",
    "class DeepUNet(nn.Module):\n",
    "    def __init__(self, in_ch=1, out_ch=1, base=32):\n",
    "        super().__init__()\n",
    "        ch = base\n",
    "        # ─ Encoder (5 스테이지) ─\n",
    "        self.e1 = ResBlock(in_ch,  ch)          # 224×224\n",
    "        self.e2 = ResBlock(ch,     ch*2)        # 112×112\n",
    "        self.e3 = ResBlock(ch*2,   ch*4)        # 56×56\n",
    "        self.e4 = ResBlock(ch*4,   ch*8)        # 28×28\n",
    "        self.e5 = ResBlock(ch*8,   ch*16)       # 14×14\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "        # ─ Bottleneck (7×7) ─\n",
    "        self.bottleneck = ResBlock(ch*16, ch*32)\n",
    "\n",
    "        # ─ Decoder ─\n",
    "        self.up5 = nn.ConvTranspose2d(ch*32, ch*16, 2, 2)  # 14×14\n",
    "        self.d5  = ResBlock(ch*32, ch*16)\n",
    "        self.se5 = SE(ch*16)\n",
    "\n",
    "        self.up4 = nn.ConvTranspose2d(ch*16, ch*8, 2, 2)   # 28×28\n",
    "        self.d4  = ResBlock(ch*16, ch*8)\n",
    "        self.se4 = SE(ch*8)\n",
    "\n",
    "        self.up3 = nn.ConvTranspose2d(ch*8, ch*4, 2, 2)    # 56×56\n",
    "        self.d3  = ResBlock(ch*8, ch*4)\n",
    "        self.se3 = SE(ch*4)\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(ch*4, ch*2, 2, 2)    # 112×112\n",
    "        self.d2  = ResBlock(ch*4, ch*2)\n",
    "        self.se2 = SE(ch*2)\n",
    "\n",
    "        self.up1 = nn.ConvTranspose2d(ch*2, ch, 2, 2)      # 224×224\n",
    "        self.d1  = ResBlock(ch*2, ch)\n",
    "        self.se1 = SE(ch)\n",
    "\n",
    "        # ─ Output ─\n",
    "        self.out_conv = nn.Conv2d(ch, out_ch, 1)\n",
    "        self.act = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        e1 = self.e1(x)\n",
    "        e2 = self.e2(self.pool(e1))\n",
    "        e3 = self.e3(self.pool(e2))\n",
    "        e4 = self.e4(self.pool(e3))\n",
    "        e5 = self.e5(self.pool(e4))\n",
    "\n",
    "        b  = self.bottleneck(self.pool(e5))\n",
    "\n",
    "        # Decoder with SE attention\n",
    "        d5 = self.se5(self.d5(torch.cat([self.up5(b), e5], dim=1)))\n",
    "        d4 = self.se4(self.d4(torch.cat([self.up4(d5), e4], dim=1)))\n",
    "        d3 = self.se3(self.d3(torch.cat([self.up3(d4), e3], dim=1)))\n",
    "        d2 = self.se2(self.d2(torch.cat([self.up2(d3), e2], dim=1)))\n",
    "        d1 = self.se1(self.d1(torch.cat([self.up1(d2), e1], dim=1)))\n",
    "\n",
    "        return self.act(self.out_conv(d1))\n",
    "\n",
    "\n",
    "class DeepDiffuser(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN을 사용하여 Diffuser의 복잡한 물리적 특성을 모델링합니다.\n",
    "    이 네트워크는 최종적으로 복소수 필드(Complex Field) H를 생성합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, N=224, latent_channels=16):\n",
    "        super().__init__()\n",
    "        # 이 네트워크는 고정된 패턴을 생성하므로, 입력 대신 학습 가능한 파라미터로 시작합니다.\n",
    "        # 이 `initial_grid`가 학습을 통해 복잡한 패턴으로 발전합니다.\n",
    "        self.initial_grid = nn.Parameter(torch.randn(1, latent_channels, N, N))\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(latent_channels, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # 최종 출력은 복소수의 실수부(real)와 허수부(imag) 2개의 채널입니다.\n",
    "            nn.Conv2d(64, 2, kernel_size=1) \n",
    "        )\n",
    "\n",
    "    def forward(self):\n",
    "        # 네트워크를 통과시켜 2채널 맵(실수부, 허수부)을 얻습니다.\n",
    "        complex_parts = self.net(self.initial_grid)\n",
    "        \n",
    "        real_part = complex_parts[:, 0, :, :]\n",
    "        imag_part = complex_parts[:, 1, :, :]\n",
    "        \n",
    "        # 두 채널을 합쳐 복소수 텐서를 만듭니다.\n",
    "        H = torch.complex(real_part, imag_part)\n",
    "        \n",
    "        # (선택 사항) 안정적인 학습을 위해 출력의 크기를 제한할 수 있습니다.\n",
    "        # 예: 진폭이 1을 넘지 않도록 정규화\n",
    "        H = H / (torch.max(torch.abs(H)) + 1e-8)\n",
    "        \n",
    "        return H.squeeze(0) # 배치 차원 제거 (N, N)\n",
    "\n",
    "class SpeckleSimulator(nn.Module):\n",
    "    def __init__(self, N=600, lam=0.532e-6, f1=0.2, f2=0.1,\n",
    "                 radius_px=10, y_shift_px=-10):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.N = N\n",
    "        self.lam = lam\n",
    "        self.f1 = f1\n",
    "        self.f2 = f2\n",
    "        self.dx = 12.5e-6\n",
    "        \n",
    "        self.sim_size = 2048 * 2\n",
    "        self.pad_size = (self.sim_size - N) // 2 \n",
    "        \n",
    "        self.sim_size2 = int(self.sim_size*1.8/2)*2\n",
    "        self.pad_size2 = (self.sim_size2 - self.sim_size) // 2 \n",
    "        \n",
    "        x = np.linspace(-self.sim_size*self.dx//2, self.sim_size*self.dx//2, self.sim_size)\n",
    "        y = x\n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        self.X = torch.from_numpy(X).cuda()\n",
    "        self.Y = torch.from_numpy(Y).cuda()\n",
    "        \n",
    "        alpha = torch.tensor(1.0)\n",
    "        self.alpha = nn.Parameter(alpha, requires_grad=True).cuda()\n",
    "        self.before_quad = torch.exp(1j * self.alpha * (self.X**2 + self.Y**2))\n",
    "        \n",
    "        # 시스템 배율 계산\n",
    "        magnification = self.f2 / self.f1\n",
    "        \n",
    "        \n",
    "        # self.radius_px = radius_px\n",
    "        # self.y_shift_px = y_shift_px\n",
    "        \n",
    "        curvature = torch.ones(N, N) * torch.pi\n",
    "        self.curvature = nn.Parameter(curvature, requires_grad=True)\n",
    "        \n",
    "        diffuser_phase = torch.ones(self.sim_size, self.sim_size) * torch.pi\n",
    "        self.diffuser = nn.Parameter(diffuser_phase, requires_grad=True)\n",
    "        \n",
    "        diffuser_amplitude = torch.ones(self.sim_size, self.sim_size)\n",
    "        self.diffuser_amplitude = nn.Parameter(diffuser_amplitude, requires_grad=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.diffuser_net = DeepDiffuser(N=self.sim_size)\n",
    "\n",
    "    def forward(self, phase):\n",
    "        H = self.diffuser_amplitude * torch.exp(1j * self.diffuser) # Set diffuser as learnable parameter\n",
    "        # H = self.diffuser_net()  # Define diffuser \n",
    "        before_quad = torch.exp(1j * self.curvature)\n",
    "        M = torch.exp(1j * phase) * before_quad         # Define field after SLM\n",
    "        M = F.pad(M, pad=(self.pad_size, self.pad_size, self.pad_size, self.pad_size), \n",
    "                               mode='constant', value=0)     # Zero-Pad the field after SLM\n",
    "        \n",
    "        \n",
    "        f = torch.fft.fftshift(torch.fft.fft2(torch.fft.ifftshift(M))) * H # Fourier transform -> multiply diffuser\n",
    "        f = F.pad(f, pad=(self.pad_size2, self.pad_size2, self.pad_size2, self.pad_size2), \n",
    "                               mode='constant', value=0)     # Zero-Pad the field after SLM\n",
    "        U = torch.fft.fftshift(torch.fft.ifft2(torch.fft.ifftshift(f)))         # Inverse Fourier Transform\n",
    "        \n",
    "        out_size = 2048\n",
    "        start = (self.sim_size2 - out_size) // 2\n",
    "        \n",
    "        I = U[..., start:start+out_size, start:start+out_size]\n",
    "        I = torch.abs(I)**2                                                      # Take Intensity\n",
    "        I = I / torch.max(I).item()                                              # Normalization (?)\n",
    "        I = torch.flip(I, dims=[-1])                                             # Flip (because it is 4f system)\n",
    "        \n",
    "        return I\n",
    "\n",
    "model = SpeckleSimulator(N=N).cuda()\n",
    "# model = DeepUNet(in_ch=1, out_ch=1, base=32).cuda()\n",
    "model = model.cuda()\n",
    "\n",
    "loss_fn = torch.nn.L1Loss()\n",
    "s = torch.tensor(1.0, requires_grad=True)\n",
    "optimizer = torch.optim.Adam(list(model.parameters()) + [s], lr=5e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "802d1c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = Image.open('apple.png').convert('L')\n",
    "image = np.array(image)\n",
    "image = cv2.resize(image, dsize=(600,600))\n",
    "image = torch.from_numpy(image).type(torch.float).cuda()\n",
    "output = model(image.unsqueeze(0).unsqueeze(0))\n",
    "output = output.squeeze(0).squeeze(0).detach().cpu().numpy()\n",
    "output = np.real(output)\n",
    "output = output / np.max(output) * 255\n",
    "cv2.imwrite('output.png', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be6972bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2048, 2048)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ea8efce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(list(model.parameters()) + [s], lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30b08f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Epoch : 0 =========\n",
      "Epoch 0, Train Loss 0.057889\n",
      "Epoch 0, Test Loss 0.048744\n",
      "========= Epoch : 1 =========\n",
      "Epoch 1, Train Loss 0.048478\n",
      "Epoch 1, Test Loss 0.045164\n",
      "========= Epoch : 2 =========\n",
      "Epoch 2, Train Loss 0.044974\n",
      "Epoch 2, Test Loss 0.042636\n",
      "========= Epoch : 3 =========\n",
      "Epoch 3, Train Loss 0.042726\n",
      "Epoch 3, Test Loss 0.041239\n",
      "========= Epoch : 4 =========\n",
      "Epoch 4, Train Loss 0.041304\n",
      "Epoch 4, Test Loss 0.039994\n",
      "========= Epoch : 5 =========\n",
      "Epoch 5, Train Loss 0.040240\n",
      "Epoch 5, Test Loss 0.039203\n",
      "========= Epoch : 6 =========\n",
      "Epoch 6, Train Loss 0.039398\n",
      "Epoch 6, Test Loss 0.038693\n",
      "========= Epoch : 7 =========\n",
      "Epoch 7, Train Loss 0.038721\n",
      "Epoch 7, Test Loss 0.038037\n",
      "========= Epoch : 8 =========\n",
      "Epoch 8, Train Loss 0.038108\n",
      "Epoch 8, Test Loss 0.037499\n",
      "========= Epoch : 9 =========\n",
      "Epoch 9, Train Loss 0.037474\n",
      "Epoch 9, Test Loss 0.037097\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 10\n",
    "for i in range(num_epoch):\n",
    "    print(f'========= Epoch : {i} =========')\n",
    "    model = model.train()\n",
    "    train_loss = []\n",
    "    for phase, speckle in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        phase = phase.type(torch.float32).cuda()\n",
    "        speckle = speckle.type(torch.float32).cuda()\n",
    "        \n",
    "        output = model(phase.unsqueeze(1))\n",
    "        output = output.squeeze(1)\n",
    "        \n",
    "        loss = loss_fn(s * output, speckle)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "    \n",
    "    print(f'Epoch {i}, Train Loss {np.mean(np.array(train_loss)):.6f}')\n",
    "    \n",
    "    model = model.eval()\n",
    "    test_loss = []\n",
    "    with torch.no_grad():\n",
    "        for phase, speckle in test_loader:\n",
    "            phase = phase.type(torch.float32).cuda()\n",
    "            speckle = speckle.type(torch.float32).cuda()\n",
    "        \n",
    "            output = model(phase.unsqueeze(1))\n",
    "            output = output.squeeze(1)\n",
    "            \n",
    "            loss = loss_fn(s * output, speckle)\n",
    "            test_loss.append(loss.item())\n",
    "        \n",
    "        print(f'Epoch {i}, Test Loss {np.mean(np.array(test_loss)):.6f}')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "695514ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b7df60c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8382754 , 0.9443062 , 0.90308607, ..., 1.1463042 , 1.1608987 ,\n",
       "        1.0075988 ],\n",
       "       [0.9637756 , 1.1901566 , 1.2052312 , ..., 1.4637276 , 1.4466708 ,\n",
       "        1.1998686 ],\n",
       "       [0.9278644 , 1.2142792 , 1.2677054 , ..., 1.4628477 , 1.4509925 ,\n",
       "        1.1965402 ],\n",
       "       ...,\n",
       "       [1.0437094 , 1.3779941 , 1.4159017 , ..., 1.2128996 , 1.1790951 ,\n",
       "        0.9268981 ],\n",
       "       [1.0599117 , 1.3712713 , 1.4160833 , ..., 1.2321908 , 1.1915606 ,\n",
       "        0.95496184],\n",
       "       [0.88641095, 1.0794909 , 1.0947812 , ..., 0.94163746, 0.930267  ,\n",
       "        0.7953366 ]], dtype=float32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diffuser = model.diffuser_amplitude.detach().cpu().numpy()\n",
    "diffuser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fcbef8f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diffuser = diffuser / np.max(diffuser) * 255\n",
    "cv2.imwrite('output.png', diffuser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "17431b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "path = r\"C:\\rkka_Projects\\physics\\dataset\\diffuser\\test\\phase\\1 (110).jpg\"\n",
    "image = Image.open(path).convert('L')\n",
    "image = np.array(image)\n",
    "image = cv2.resize(image, dsize=(N,N))\n",
    "phase = image / 255 * 2 * np.pi\n",
    "\n",
    "phase = torch.from_numpy(phase).cuda()\n",
    "phase = phase.type(torch.float32)\n",
    "phase = phase.unsqueeze(0).unsqueeze(0)\n",
    "output = model(phase)\n",
    "\n",
    "output = output.squeeze(0).squeeze(0)\n",
    "output = output.detach().cpu().numpy()\n",
    "output = output / np.max(output) * 255\n",
    "Image.fromarray(output.astype('uint8')).save('output.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "b3e8ff8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open('apple.png').convert('L')\n",
    "image = np.array(image)\n",
    "image = cv2.resize(image, dsize=(224,224))\n",
    "phase = image / 255 * 2 * np.pi\n",
    "\n",
    "speckle = create_speckle(phase)\n",
    "speckle = speckle / np.max(speckle) * 225\n",
    "Image.fromarray(speckle.astype('uint8')).save(f'speckle.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877a913d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# =========================================================================================\n",
    "# ======================== 수정된 IterativeInverter 모델 ====================================\n",
    "# =========================================================================================\n",
    "\n",
    "class IterativeInverter(nn.Module):\n",
    "    def __init__(self, forward_model: SpeckleSimulator):\n",
    "        super().__init__()\n",
    "        # 1. forward_model에서 학습된 고정 파라미터를 가져와 buffer로 등록합니다.\n",
    "        #    이 파라미터들은 더 이상 학습되지 않습니다.\n",
    "        self.N = forward_model.N\n",
    "        self.sim_size = forward_model.sim_size            # 4096\n",
    "        self.pad_size = forward_model.pad_size            # 1936\n",
    "        self.sim_size2 = forward_model.sim_size2          # 7372\n",
    "        self.pad_size2 = forward_model.pad_size2          # 1638\n",
    "        self.out_size = 2048  # SpeckleSimulator의 crop 크기\n",
    "\n",
    "        # 디바이스 일관성을 위해 forward_model의 파라미터로부터 디바이스 정보를 가져옵니다.\n",
    "        self.device = forward_model.curvature.device\n",
    "        \n",
    "        # 학습된 파라미터들을 buffer로 등록\n",
    "        self.register_buffer('curvature', forward_model.curvature.detach())\n",
    "        self.register_buffer('diffuser_phase', forward_model.diffuser.detach())\n",
    "        self.register_buffer('diffuser_amplitude', forward_model.diffuser_amplitude.detach())\n",
    "        \n",
    "        # 2. Diffuser의 진폭과 위상을 포함하는 시스템 함수 H를 미리 계산합니다.\n",
    "        H = self.diffuser_amplitude * torch.exp(1j * self.diffuser_phase)\n",
    "        self.register_buffer('H', H)\n",
    "        \n",
    "        # 3. H의 역연산에 사용할 켤레 복소수(complex conjugate)를 계산합니다.\n",
    "        epsilon = 1e-8\n",
    "        self.register_buffer('H_inv', torch.conj(self.H) / (torch.abs(self.H)**2 + epsilon))\n",
    "\n",
    "    def forward_propagation(self, input_phase):\n",
    "        \"\"\" 입력 위상 -> 출력 복소수 필드 (SpeckleSimulator의 과정을 정확히 재현) \"\"\"\n",
    "        # 1. 입력단(SLM): 입력 위상과 학습된 곡률(curvature)을 곱함\n",
    "        before_quad = torch.exp(1j * self.curvature)\n",
    "        M = torch.exp(1j * input_phase) * before_quad\n",
    "        \n",
    "        # 2. 첫 번째 패딩: N -> sim_size (224 -> 4096)\n",
    "        M_padded = F.pad(M, pad=(self.pad_size, self.pad_size, self.pad_size, self.pad_size), \n",
    "                         mode='constant', value=0)\n",
    "        \n",
    "        # 3. 첫 번째 렌즈 및 Diffuser: FFT 후 H와 곱함\n",
    "        f = torch.fft.fftshift(torch.fft.fft2(torch.fft.ifftshift(M_padded))) * self.H\n",
    "        \n",
    "        # 4. 두 번째 패딩: sim_size -> sim_size2 (4096 -> 7372)\n",
    "        f_padded = F.pad(f, pad=(self.pad_size2, self.pad_size2, self.pad_size2, self.pad_size2), \n",
    "                         mode='constant', value=0)\n",
    "        \n",
    "        # 5. 두 번째 렌즈: IFFT\n",
    "        U_large = torch.fft.fftshift(torch.fft.ifft2(torch.fft.ifftshift(f_padded)))\n",
    "        \n",
    "        # 6. 출력단 크롭: sim_size2 -> out_size (7372 -> 2048)\n",
    "        start = (self.sim_size2 - self.out_size) // 2\n",
    "        U_cropped = U_large[..., start:start+self.out_size, start:start+self.out_size]\n",
    "        \n",
    "        # 7. 4f 시스템 뒤집힘(flip) 적용\n",
    "        U_flipped = torch.flip(U_cropped, dims=[-1])\n",
    "        \n",
    "        # 8. 최종 다운샘플링: out_size -> N (2048 -> 600)\n",
    "        #    interpolate는 (B, C, H, W) 4D 텐서를 요구하므로 차원 확장 후 적용\n",
    "        U_real = U_flipped.real.unsqueeze(0).unsqueeze(0)\n",
    "        U_imag = U_flipped.imag.unsqueeze(0).unsqueeze(0)\n",
    "        U_real_small = F.interpolate(U_real, size=(self.N, self.N), mode='area')\n",
    "        U_imag_small = F.interpolate(U_imag, size=(self.N, self.N), mode='area')\n",
    "        \n",
    "        # 다시 복소수 필드로 합치고 차원 축소\n",
    "        output_field = torch.complex(U_real_small, U_imag_small).squeeze(0).squeeze(0)\n",
    "        \n",
    "        return output_field\n",
    "\n",
    "    def backward_propagation(self, output_field):\n",
    "        \"\"\" 출력 필드 -> 입력 필드 (Forward Propagation의 모든 과정을 정확히 역순으로 수행) \"\"\"\n",
    "        # 8. 역-다운샘플링: N -> out_size (600 -> 2048)\n",
    "        U_real_small = output_field.real.unsqueeze(0).unsqueeze(0)\n",
    "        U_imag_small = output_field.imag.unsqueeze(0).unsqueeze(0)\n",
    "        # 업샘플링에는 'bicubic' 이나 'bilinear'가 적합\n",
    "        U_real_large = F.interpolate(U_real_small, size=(self.out_size, self.out_size), mode='bicubic', align_corners=False)\n",
    "        U_imag_large = F.interpolate(U_imag_small, size=(self.out_size, self.out_size), mode='bicubic', align_corners=False)\n",
    "        U_cropped = torch.complex(U_real_large, U_imag_large).squeeze(0).squeeze(0)\n",
    "\n",
    "        # 7. 역-뒤집힘 적용\n",
    "        U_unflipped = torch.flip(U_cropped, dims=[-1])\n",
    "\n",
    "        # 6. 역-크롭: out_size를 sim_size2 캔버스에 다시 삽입\n",
    "        start = (self.sim_size2 - self.out_size) // 2\n",
    "        U_large = torch.zeros(output_field.shape[:-2] + (self.sim_size2, self.sim_size2),\n",
    "                              dtype=torch.cfloat, device=output_field.device)\n",
    "        U_large[..., start:start+self.out_size, start:start+self.out_size] = U_unflipped\n",
    "        \n",
    "        # 5. 역-IFFT (-> FFT)\n",
    "        f_padded = torch.fft.fftshift(torch.fft.fft2(torch.fft.ifftshift(U_large)))\n",
    "\n",
    "        # 4. 역-두 번째 패딩: sim_size2 -> sim_size 크롭\n",
    "        f = f_padded[..., self.pad_size2:-self.pad_size2, self.pad_size2:-self.pad_size2]\n",
    "\n",
    "        # 3. 역-H 곱셈 (-> H_inv 곱셈)\n",
    "        M_padded_fourier = f * self.H_inv\n",
    "        \n",
    "        # 2. 역-FFT (-> IFFT)\n",
    "        M_padded = torch.fft.ifftshift(torch.fft.ifft2(torch.fft.fftshift(M_padded_fourier)))\n",
    "        \n",
    "        # 1. 역-첫 번째 패딩: sim_size -> N 크롭\n",
    "        estimated_input_field = M_padded[..., self.pad_size:-self.pad_size, self.pad_size:-self.pad_size]\n",
    "\n",
    "        return estimated_input_field\n",
    "\n",
    "    def find_phase(self, target_image, iterations=50):\n",
    "        \"\"\" Gerchberg-Saxton 알고리즘으로 목표 이미지를 만드는 최적의 위상을 찾습니다 \"\"\"\n",
    "        # 목표 진폭 계산 (0으로 나누는 것을 방지)\n",
    "        target_amplitude = torch.sqrt(target_image.clamp(min=1e-8))\n",
    "        \n",
    "        # 시작점: 입력 위상을 랜덤하게 초기화\n",
    "        input_phase = (torch.rand_like(target_image) * 2 * torch.pi).to(self.device)\n",
    "\n",
    "        for _ in range(iterations):\n",
    "            # 1. [정방향] 현재 위상으로 출력 필드 계산\n",
    "            output_field = self.forward_propagation(input_phase)\n",
    "            \n",
    "            # 2. [출력 평면 제약] 계산된 위상은 유지하되, 진폭은 목표 진폭으로 강제 교체\n",
    "            estimated_phase_at_output = torch.angle(output_field)\n",
    "            corrected_output_field = target_amplitude * torch.exp(1j * estimated_phase_at_output)\n",
    "            \n",
    "            # 3. [역방향] 수정된 출력 필드를 역전파하여 입력 필드 추정\n",
    "            estimated_input_field = self.backward_propagation(corrected_output_field)\n",
    "            \n",
    "            # 4. [입력 평면 제약] SLM은 위상만 조절 가능하므로, 추정된 입력 필드의 위상만 취함\n",
    "            #    ★★ 중요 ★★: 추정된 필드는 (입력 위상 + 곡률)이므로, 곡률을 빼주어야 순수 입력 위상을 얻을 수 있습니다.\n",
    "            input_phase = torch.angle(estimated_input_field) - self.curvature\n",
    "\n",
    "        return input_phase.detach() # 최종적으로 찾아낸 위상을 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4d4fec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최적의 위상 패턴을 찾는 중... (반복 최적화)\n",
      "완료!\n"
     ]
    }
   ],
   "source": [
    "# 학습된 모델을 'forward_model'로 저장\n",
    "forward_model = model.eval()\n",
    "\n",
    "# 1. 학습된 forward_model로 Inverter 인스턴스 생성\n",
    "inverter = IterativeInverter(forward_model).to(device)\n",
    "\n",
    "# 2. 목표 이미지 준비 (이전과 동일)\n",
    "target_img = Image.open('square.jpg').convert('L')\n",
    "target_img = np.array(target_img)\n",
    "target_ima = 255 - target_img\n",
    "target_img = cv2.resize(target_img, dsize=(600, 600))\n",
    "target_img = target_img / np.max(target_img) * 2 * np.pi\n",
    "target_image = torch.from_numpy(target_img).to(device, dtype=torch.float32)\n",
    "\n",
    "# 3. Inverter를 사용하여 최적 위상 찾기\n",
    "print(\"최적의 위상 패턴을 찾는 중... (반복 최적화)\")\n",
    "# .find_phase() 메서드를 호출하면 내부적으로 루프가 돌면서 위상을 찾아줍니다.\n",
    "optimized_phase = inverter.find_phase(target_image, iterations=200)\n",
    "print(\"완료!\")\n",
    "\n",
    "# 4. 결과 검증: 찾은 위상을 원래의 forward_model에 넣어 사과가 나오는지 확인\n",
    "with torch.no_grad():\n",
    "    # 찾은 위상을 정방향 시뮬레이터에 입력\n",
    "    final_output = forward_model(optimized_phase.unsqueeze(0).unsqueeze(0))\n",
    "    final_output = final_output.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f2d3e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = optimized_phase.detach().cpu().numpy()\n",
    "temp = temp / np.max(temp) * 255\n",
    "Image.fromarray(temp.astype('uint8')).save('phase.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "641c5321",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = final_output.detach().cpu().numpy()\n",
    "temp = temp / np.max(temp) * 255\n",
    "Image.fromarray(temp.astype('uint8')).save('result.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "holo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
