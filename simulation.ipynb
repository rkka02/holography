{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e69e88e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57506165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = np.zeros((600,600))\n",
    "cv2.imwrite('dataset/zeros.png', pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a50adf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "N = 224\n",
    "random_phase = np.random.rand(4*N, 4*N) * 2 * np.pi\n",
    "def create_speckle(phase, lam=0.532e-6, f1=0.20, f2=0.10):\n",
    "    N   = phase.shape[0]\n",
    "    Np  = 4 * N                    # 패딩된 격자 크기 # 4 * 224 = 896\n",
    "    pad = (Np - N)//2              # (896 - 224)//2 = 336\n",
    "    \n",
    "    # 1) 입력면\n",
    "    M = np.exp(1j * phase)\n",
    "    M = np.pad(M, pad_width=pad, mode='constant', constant_values=0)\n",
    "    \n",
    "    # 2) 푸리에면 무작위 위상 디퓨저\n",
    "    D = np.exp(1j * random_phase)          # 위상만 가진 디퓨저\n",
    "    \n",
    "    # 3) 1차 FFT (푸리에면)\n",
    "    F = np.fft.fftshift(np.fft.fft2(np.fft.fftshift(M))) * D\n",
    "    \n",
    "    # 4) 2차 FFT (영상면)\n",
    "    U = np.fft.ifftshift(np.fft.ifft2(np.fft.ifftshift(F)))\n",
    "    \n",
    "    # 배율 보정 & 뒤집기\n",
    "    U *= (1/(lam**2 * f1 * f2))\n",
    "    U = np.flip(U, axis=(0, 1))\n",
    "    \n",
    "    # 5) 결과 크롭 (원본보다 2배 넓게 보기)\n",
    "    out_size = 2*N           # 예: 2N×2N ROI # 2*224=448\n",
    "    start = (Np - out_size)//2\n",
    "    I = np.abs(U[start:start+out_size, start:start+out_size])**2\n",
    "    \n",
    "    # 필요하면 다시 0.5배 축소(실제 배율) 후 붙여넣기\n",
    "    I_small = cv2.resize(I, dsize=(N, N), interpolation=cv2.INTER_AREA)\n",
    "    I_small = I_small / np.max(I_small[N//4:3*N//4, N//4:3*N//4])\n",
    "    return I_small\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73cf687b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:08<00:00, 12.35it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "path = 'dataset/train/phase'\n",
    "vmax =  2\n",
    "for file in tqdm(os.listdir(path)):\n",
    "    phase = Image.open(os.path.join(path, file)).convert('L')\n",
    "    phase = np.array(phase)\n",
    "    phase = phase / 255 * 2 * np.pi\n",
    "    phase = cv2.resize(phase, dsize=(224, 224))\n",
    "    speckle = create_speckle(phase)\n",
    "    speckle = speckle / vmax * 225\n",
    "    Image.fromarray(speckle.astype('uint8')).save(f'dataset/train/speckle/{file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27d11d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1단계: 전체 데이터셋의 99.9 퍼센타일 값을 계산합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:08<00:00, 12.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "계산된 99.9 퍼센타일 (vmax): 1.9363751369183269\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "# create_speckle 함수가 정의되어 있다고 가정합니다.\n",
    "# 예시: def create_speckle(phase): ... return speckle_pattern\n",
    "\n",
    "# --- 경로 설정 ---\n",
    "path = 'dataset/train/phase'\n",
    "\n",
    "all_speckle_values = []\n",
    "\n",
    "print(\"1단계: 전체 데이터셋의 99.9 퍼센타일 값을 계산합니다...\")\n",
    "for file in tqdm(os.listdir(path)):\n",
    "    # 이미지를 불러오고 phase 맵으로 변환\n",
    "    phase = Image.open(os.path.join(path, file)).convert('L')\n",
    "    phase = np.array(phase)\n",
    "    phase = phase / 255.0 * 2 * np.pi\n",
    "    phase = cv2.resize(phase, dsize=(224, 224))\n",
    "\n",
    "    # 스페클 생성 (이 함수는 이미 정의되어 있어야 합니다)\n",
    "    speckle = create_speckle(phase)\n",
    "\n",
    "    # 생성된 스페클 값들을 하나의 리스트에 모두 추가\n",
    "    all_speckle_values.extend(speckle.flatten())\n",
    "\n",
    "# 모든 스페클 값들 중에서 99.9 퍼센타일 계산\n",
    "# 데이터셋이 매우 크면 이 부분에서 메모리를 많이 사용할 수 있습니다.\n",
    "vmax_p999 = np.percentile(all_speckle_values, 99.99)\n",
    "\n",
    "print(f\"\\n계산된 99.9 퍼센타일 (vmax): {vmax_p999}\")\n",
    "\n",
    "# 이제 이 vmax_p999 값을 2단계에서 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8664374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "def get_slm_grey_level(desired_phase, calibration_data):\n",
    "\n",
    "    if not calibration_data:\n",
    "        raise ValueError(\"SLM calibration data cannot be empty.\")\n",
    "    \n",
    "    # 그레이 레벨 배열 (0, 1, 2, ..., 255)\n",
    "    grey_levels = np.arange(len(calibration_data))\n",
    "\n",
    "    # interp1d 함수를 사용하여 보간 함수 생성\n",
    "    # x: 위상 값 (calibration_data), y: 그레이 레벨 (grey_levels)\n",
    "    # kind='linear': 선형 보간\n",
    "    # bounds_error=False: 입력 위상이 calibration_data 범위를 벗어나도 에러 발생 안 함\n",
    "    # fill_value=(grey_levels[0], grey_levels[-1]): 범위 밖의 값은 최소/최대 그레이 레벨로 클램프\n",
    "    interpolation_function = interp1d(\n",
    "        calibration_data,\n",
    "        grey_levels,\n",
    "        kind='linear',\n",
    "        bounds_error=False,\n",
    "        fill_value=(grey_levels[0], grey_levels[-1])\n",
    "    )\n",
    "\n",
    "    # 원하는 위상에 해당하는 그레이 레벨 계산\n",
    "    interpolated_grey_level = interpolation_function(desired_phase)\n",
    "\n",
    "    # 결과를 가장 가까운 정수로 반올림하고, 정수형으로 변환하여 반환\n",
    "    # .item()은 numpy 배열이 아닌 스칼라 값을 반환하도록 합니다.\n",
    "    return int(np.round(interpolated_grey_level).item())\n",
    "\n",
    "loaded_data = []\n",
    "with open('LUT.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        stripped_line = line.strip() # 공백(줄바꿈 포함) 제거\n",
    "        if stripped_line: # 빈 줄이 아닌 경우에만 처리\n",
    "            loaded_data.append(float(stripped_line))\n",
    "            \n",
    "grey_levels = np.arange(len(loaded_data))\n",
    "\n",
    "phase_to_grey_level_interpolator = interp1d(\n",
    "    loaded_data,\n",
    "    grey_levels,\n",
    "    kind='linear',\n",
    "    bounds_error=False,\n",
    "    fill_value=(grey_levels[0], grey_levels[-1])\n",
    ")\n",
    "\n",
    "grey_to_phase_level_interpolator = interp1d(\n",
    "    grey_levels,\n",
    "    loaded_data,\n",
    "    kind='linear',\n",
    "    bounds_error=False,\n",
    "    fill_value=(grey_levels[0], grey_levels[-1])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3d15bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# --- 기본 설정 (이전과 동일) ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "N = 600\n",
    "LEARNING_RATE_MODEL = 1e-4\n",
    "LEARNING_RATE_PHASE = 1e-3\n",
    "\n",
    "\n",
    "# --- 🌟 데이터셋 클래스 정의 🌟 ---\n",
    "class SpeckleDataset(Dataset):\n",
    "    def __init__(self, phase_dir, speckle_dir):\n",
    "        # 이미지 파일 경로 리스트 가져오기\n",
    "        self.phase_paths = sorted(os.listdir(phase_dir))\n",
    "        self.speckle_paths = sorted(os.listdir(speckle_dir))\n",
    "\n",
    "        self.phase_list = []\n",
    "        self.speckle_list = []\n",
    "\n",
    "        # 각 이미지에 대한 위상을 저장할 딕셔너리\n",
    "        for path in self.phase_paths:\n",
    "            phase = Image.open(os.path.join(phase_dir, path)).convert('L')\n",
    "            phase = np.array(phase)\n",
    "            \n",
    "            phase = cv2.resize(phase, dsize=(N, N))\n",
    "            phase = phase / np.max(phase) * 2 * np.pi\n",
    "            phase = np.clip(phase_to_grey_level_interpolator(phase), 0, 255).astype('uint8')\n",
    "            \n",
    "            phase = grey_to_phase_level_interpolator(phase)\n",
    "            # phase = np.pad(phase, pad_width=(2048-N)//2, mode='constant', constant_values=0)\n",
    "            self.phase_list.append(phase)\n",
    "        \n",
    "        for path in self.speckle_paths:\n",
    "            speckle = Image.open(os.path.join(speckle_dir, path)).convert('L')\n",
    "            speckle = np.array(speckle)\n",
    "            speckle = speckle / np.max(speckle)\n",
    "            # speckle = cv2.resize(speckle, dsize=(N, N))\n",
    "            self.speckle_list.append(speckle)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.phase_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        phase = self.phase_list[idx]\n",
    "        speckle = self.speckle_list[idx]\n",
    "        phase = torch.from_numpy(phase)\n",
    "        speckle = torch.from_numpy(speckle)\n",
    "        return phase, speckle\n",
    "    \n",
    "train_dataset = SpeckleDataset('dataset/diffuser/train/phase', 'dataset/diffuser/train/speckle')\n",
    "test_dataset = SpeckleDataset('dataset/diffuser/test/phase', 'dataset/diffuser/test/speckle')\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "02656b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_msssim import ssim, ms_ssim # <--- 이 부분을 추가하세요\n",
    "import torch.nn.functional as F\n",
    "from skimage import feature\n",
    "\n",
    "# --- 기본 설정 (이전과 동일) ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "N = 600\n",
    "LEARNING_RATE_MODEL = 1e-4\n",
    "LEARNING_RATE_PHASE = 1e-3\n",
    "\n",
    "import torch, math\n",
    "import torch.nn as nn\n",
    "\n",
    "# ───────────────── ① 고정 pupil 생성 ─────────────────\n",
    "def make_pupil(N, radius_px=10, y_shift_px=-10):\n",
    "    x = torch.arange(-N//2, N//2)\n",
    "    X, Y = torch.meshgrid(x, x, indexing='ij')\n",
    "    P = (((X+y_shift_px)**2 + Y**2) < radius_px**2).float()\n",
    "    return P          # (N,N) real, 0/1\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# ─────────────────── Residual DoubleConv ───────────────────\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "        )\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        # 입력·출력 채널이 다르면 1×1 프로젝션\n",
    "        self.skip = nn.Identity() if in_ch == out_ch else nn.Conv2d(in_ch, out_ch, 1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.conv(x) + self.skip(x))\n",
    "\n",
    "# ─────────────────── Channel Attention (Squeeze & Excite) ───────────────────\n",
    "class SE(nn.Module):\n",
    "    def __init__(self, ch, r=8):\n",
    "        super().__init__()\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc   = nn.Sequential(nn.Linear(ch, ch//r, bias=False),\n",
    "                                  nn.ReLU(inplace=True),\n",
    "                                  nn.Linear(ch//r, ch, bias=False),\n",
    "                                  nn.Sigmoid())\n",
    "    def forward(self, x):\n",
    "        b,c,_,_ = x.shape\n",
    "        w = self.pool(x).view(b, c)\n",
    "        w = self.fc(w).view(b, c, 1, 1)\n",
    "        return x * w\n",
    "\n",
    "# ─────────────────── Deep U-Net ───────────────────\n",
    "class DeepUNet(nn.Module):\n",
    "    def __init__(self, in_ch=1, out_ch=1, base=32):\n",
    "        super().__init__()\n",
    "        ch = base\n",
    "        # ─ Encoder (5 스테이지) ─\n",
    "        self.e1 = ResBlock(in_ch,  ch)          # 224×224\n",
    "        self.e2 = ResBlock(ch,     ch*2)        # 112×112\n",
    "        self.e3 = ResBlock(ch*2,   ch*4)        # 56×56\n",
    "        self.e4 = ResBlock(ch*4,   ch*8)        # 28×28\n",
    "        self.e5 = ResBlock(ch*8,   ch*16)       # 14×14\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "        # ─ Bottleneck (7×7) ─\n",
    "        self.bottleneck = ResBlock(ch*16, ch*32)\n",
    "\n",
    "        # ─ Decoder ─\n",
    "        self.up5 = nn.ConvTranspose2d(ch*32, ch*16, 2, 2)  # 14×14\n",
    "        self.d5  = ResBlock(ch*32, ch*16)\n",
    "        self.se5 = SE(ch*16)\n",
    "\n",
    "        self.up4 = nn.ConvTranspose2d(ch*16, ch*8, 2, 2)   # 28×28\n",
    "        self.d4  = ResBlock(ch*16, ch*8)\n",
    "        self.se4 = SE(ch*8)\n",
    "\n",
    "        self.up3 = nn.ConvTranspose2d(ch*8, ch*4, 2, 2)    # 56×56\n",
    "        self.d3  = ResBlock(ch*8, ch*4)\n",
    "        self.se3 = SE(ch*4)\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(ch*4, ch*2, 2, 2)    # 112×112\n",
    "        self.d2  = ResBlock(ch*4, ch*2)\n",
    "        self.se2 = SE(ch*2)\n",
    "\n",
    "        self.up1 = nn.ConvTranspose2d(ch*2, ch, 2, 2)      # 224×224\n",
    "        self.d1  = ResBlock(ch*2, ch)\n",
    "        self.se1 = SE(ch)\n",
    "\n",
    "        # ─ Output ─\n",
    "        self.out_conv = nn.Conv2d(ch, out_ch, 1)\n",
    "        self.act = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        e1 = self.e1(x)\n",
    "        e2 = self.e2(self.pool(e1))\n",
    "        e3 = self.e3(self.pool(e2))\n",
    "        e4 = self.e4(self.pool(e3))\n",
    "        e5 = self.e5(self.pool(e4))\n",
    "\n",
    "        b  = self.bottleneck(self.pool(e5))\n",
    "\n",
    "        # Decoder with SE attention\n",
    "        d5 = self.se5(self.d5(torch.cat([self.up5(b), e5], dim=1)))\n",
    "        d4 = self.se4(self.d4(torch.cat([self.up4(d5), e4], dim=1)))\n",
    "        d3 = self.se3(self.d3(torch.cat([self.up3(d4), e3], dim=1)))\n",
    "        d2 = self.se2(self.d2(torch.cat([self.up2(d3), e2], dim=1)))\n",
    "        d1 = self.se1(self.d1(torch.cat([self.up1(d2), e1], dim=1)))\n",
    "\n",
    "        return self.act(self.out_conv(d1))\n",
    "\n",
    "\n",
    "class DeepDiffuser(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN을 사용하여 Diffuser의 복잡한 물리적 특성을 모델링합니다.\n",
    "    이 네트워크는 최종적으로 복소수 필드(Complex Field) H를 생성합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, N=224, latent_channels=16):\n",
    "        super().__init__()\n",
    "        # 이 네트워크는 고정된 패턴을 생성하므로, 입력 대신 학습 가능한 파라미터로 시작합니다.\n",
    "        # 이 `initial_grid`가 학습을 통해 복잡한 패턴으로 발전합니다.\n",
    "        self.initial_grid = nn.Parameter(torch.randn(1, latent_channels, N, N))\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(latent_channels, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # 최종 출력은 복소수의 실수부(real)와 허수부(imag) 2개의 채널입니다.\n",
    "            nn.Conv2d(64, 2, kernel_size=1) \n",
    "        )\n",
    "\n",
    "    def forward(self):\n",
    "        # 네트워크를 통과시켜 2채널 맵(실수부, 허수부)을 얻습니다.\n",
    "        complex_parts = self.net(self.initial_grid)\n",
    "        \n",
    "        real_part = complex_parts[:, 0, :, :]\n",
    "        imag_part = complex_parts[:, 1, :, :]\n",
    "        \n",
    "        # 두 채널을 합쳐 복소수 텐서를 만듭니다.\n",
    "        H = torch.complex(real_part, imag_part)\n",
    "        \n",
    "        # (선택 사항) 안정적인 학습을 위해 출력의 크기를 제한할 수 있습니다.\n",
    "        # 예: 진폭이 1을 넘지 않도록 정규화\n",
    "        H = H / (torch.max(torch.abs(H)) + 1e-8)\n",
    "        \n",
    "        return H.squeeze(0) # 배치 차원 제거 (N, N)\n",
    "\n",
    "class SpeckleSimulator(nn.Module):\n",
    "    def __init__(self, N=600, lam=0.532e-6, f1=0.2, f2=0.1,\n",
    "                 radius_px=10, y_shift_px=-10):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.N = N\n",
    "        self.lam = lam\n",
    "        self.f1 = f1\n",
    "        self.f2 = f2\n",
    "        self.dx = 12.5e-6\n",
    "        \n",
    "        self.sim_size = 2048 * 2\n",
    "        self.pad_size = (self.sim_size - N) // 2 \n",
    "        \n",
    "        self.sim_size2 = int(self.sim_size*1.8/2)*2\n",
    "        self.pad_size2 = (self.sim_size2 - self.sim_size) // 2 \n",
    "        \n",
    "        x = np.linspace(-self.sim_size*self.dx//2, self.sim_size*self.dx//2, self.sim_size)\n",
    "        y = x\n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        self.X = torch.from_numpy(X).cuda()\n",
    "        self.Y = torch.from_numpy(Y).cuda()\n",
    "        \n",
    "        alpha = torch.tensor(1.0)\n",
    "        self.alpha = nn.Parameter(alpha, requires_grad=True).cuda()\n",
    "        self.before_quad = torch.exp(1j * self.alpha * (self.X**2 + self.Y**2))\n",
    "        \n",
    "        # 시스템 배율 계산\n",
    "        magnification = self.f2 / self.f1\n",
    "        \n",
    "        \n",
    "        # self.radius_px = radius_px\n",
    "        # self.y_shift_px = y_shift_px\n",
    "        \n",
    "        curvature = torch.ones(N, N) * torch.pi\n",
    "        self.curvature = nn.Parameter(curvature, requires_grad=True)\n",
    "        self.before_quad = torch.exp(1j * self.curvature)\n",
    "        \n",
    "        diffuser_phase = torch.ones(self.sim_size, self.sim_size) * torch.pi\n",
    "        self.diffuser = nn.Parameter(diffuser_phase, requires_grad=True)\n",
    "        \n",
    "        diffuser_amplitude = torch.ones(self.sim_size, self.sim_size)\n",
    "        self.diffuser_amplitude = nn.Parameter(diffuser_amplitude, requires_grad=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.diffuser_net = DeepDiffuser(N=self.sim_size)\n",
    "\n",
    "    def forward(self, phase):\n",
    "        H = self.diffuser_amplitude * torch.exp(1j * self.diffuser) # Set diffuser as learnable parameter\n",
    "        # H = self.diffuser_net()  # Define diffuser \n",
    "        before_quad = torch.exp(1j * self.curvature)\n",
    "        M = torch.exp(1j * phase) * before_quad         # Define field after SLM\n",
    "        M = F.pad(M, pad=(self.pad_size, self.pad_size, self.pad_size, self.pad_size), \n",
    "                               mode='constant', value=0)     # Zero-Pad the field after SLM\n",
    "        \n",
    "        \n",
    "        f = torch.fft.fftshift(torch.fft.fft2(torch.fft.ifftshift(M))) * H # Fourier transform -> multiply diffuser\n",
    "        f = F.pad(f, pad=(self.pad_size2, self.pad_size2, self.pad_size2, self.pad_size2), \n",
    "                               mode='constant', value=0)     # Zero-Pad the field after SLM\n",
    "        U = torch.fft.fftshift(torch.fft.ifft2(torch.fft.ifftshift(f)))         # Inverse Fourier Transform\n",
    "        \n",
    "        out_size = 2048\n",
    "        start = (self.sim_size2 - out_size) // 2\n",
    "        \n",
    "        I = U[..., start:start+out_size, start:start+out_size]\n",
    "        I = torch.abs(I)**2                                                      # Take Intensity\n",
    "        I = I / torch.max(I).item()                                              # Normalization (?)\n",
    "        I = torch.flip(I, dims=[-1])                                             # Flip (because it is 4f system)\n",
    "        \n",
    "        return I\n",
    "\n",
    "model = SpeckleSimulator(N=N).cuda()\n",
    "# model = DeepUNet(in_ch=1, out_ch=1, base=32).cuda()\n",
    "model = model.cuda()\n",
    "\n",
    "loss_fn = torch.nn.L1Loss()\n",
    "s = torch.tensor(1.0, requires_grad=True)\n",
    "optimizer = torch.optim.Adam(list(model.parameters()) + [s], lr=5e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "802d1c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = Image.open('apple.png').convert('L')\n",
    "image = np.array(image)\n",
    "image = cv2.resize(image, dsize=(600,600))\n",
    "image = torch.from_numpy(image).type(torch.float).cuda()\n",
    "output = model(image.unsqueeze(0).unsqueeze(0))\n",
    "output = output.squeeze(0).squeeze(0).detach().cpu().numpy()\n",
    "output = np.real(output)\n",
    "output = output / np.max(output) * 255\n",
    "cv2.imwrite('output.png', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "be6972bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ea8efce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(list(model.parameters()) + [s], lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "30b08f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Epoch : 0 =========\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m phase \u001b[38;5;241m=\u001b[39m phase\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m      9\u001b[0m speckle \u001b[38;5;241m=\u001b[39m speckle\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m---> 11\u001b[0m output \u001b[38;5;241m=\u001b[39m model(phase\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     12\u001b[0m output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(s \u001b[38;5;241m*\u001b[39m output, speckle)\n",
      "File \u001b[1;32mc:\\miniconda3\\envs\\holo\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\miniconda3\\envs\\holo\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[35], line 226\u001b[0m, in \u001b[0;36mSpeckleSimulator.forward\u001b[1;34m(self, phase)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;66;03m# H = self.diffuser_net()  # Define diffuser \u001b[39;00m\n\u001b[0;32m    225\u001b[0m before_quad \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m1\u001b[39mj \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurvature)\n\u001b[1;32m--> 226\u001b[0m M \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m1\u001b[39mj \u001b[38;5;241m*\u001b[39m phase) \u001b[38;5;241m*\u001b[39m before_quad         \u001b[38;5;66;03m# Define field after SLM\u001b[39;00m\n\u001b[0;32m    227\u001b[0m M \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(M, pad\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_size), \n\u001b[0;32m    228\u001b[0m                        mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m'\u001b[39m, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)     \u001b[38;5;66;03m# Zero-Pad the field after SLM\u001b[39;00m\n\u001b[0;32m    231\u001b[0m f \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfft\u001b[38;5;241m.\u001b[39mfftshift(torch\u001b[38;5;241m.\u001b[39mfft\u001b[38;5;241m.\u001b[39mfft2(torch\u001b[38;5;241m.\u001b[39mfft\u001b[38;5;241m.\u001b[39mifftshift(M))) \u001b[38;5;241m*\u001b[39m H \u001b[38;5;66;03m# Fourier transform -> multiply diffuser\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epoch = 10\n",
    "for i in range(num_epoch):\n",
    "    print(f'========= Epoch : {i} =========')\n",
    "    model = model.train()\n",
    "    train_loss = []\n",
    "    for phase, speckle in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        phase = phase.type(torch.float32).cuda()\n",
    "        speckle = speckle.type(torch.float32).cuda()\n",
    "        \n",
    "        output = model(phase.unsqueeze(1))\n",
    "        output = output.squeeze(1)\n",
    "        \n",
    "        loss = loss_fn(s * output, speckle)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "    \n",
    "    print(f'Epoch {i}, Train Loss {np.mean(np.array(train_loss)):.6f}')\n",
    "    \n",
    "    model = model.eval()\n",
    "    test_loss = []\n",
    "    with torch.no_grad():\n",
    "        for phase, speckle in test_loader:\n",
    "            phase = phase.type(torch.float32).cuda()\n",
    "            speckle = speckle.type(torch.float32).cuda()\n",
    "        \n",
    "            output = model(phase.unsqueeze(1))\n",
    "            output = output.squeeze(1)\n",
    "            \n",
    "            loss = loss_fn(s * output, speckle)\n",
    "            test_loss.append(loss.item())\n",
    "        \n",
    "        print(f'Epoch {i}, Test Loss {np.mean(np.array(test_loss)):.6f}')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "695514ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b7df60c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8382754 , 0.9443062 , 0.90308607, ..., 1.1463042 , 1.1608987 ,\n",
       "        1.0075988 ],\n",
       "       [0.9637756 , 1.1901566 , 1.2052312 , ..., 1.4637276 , 1.4466708 ,\n",
       "        1.1998686 ],\n",
       "       [0.9278644 , 1.2142792 , 1.2677054 , ..., 1.4628477 , 1.4509925 ,\n",
       "        1.1965402 ],\n",
       "       ...,\n",
       "       [1.0437094 , 1.3779941 , 1.4159017 , ..., 1.2128996 , 1.1790951 ,\n",
       "        0.9268981 ],\n",
       "       [1.0599117 , 1.3712713 , 1.4160833 , ..., 1.2321908 , 1.1915606 ,\n",
       "        0.95496184],\n",
       "       [0.88641095, 1.0794909 , 1.0947812 , ..., 0.94163746, 0.930267  ,\n",
       "        0.7953366 ]], dtype=float32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diffuser = model.diffuser_amplitude.detach().cpu().numpy()\n",
    "diffuser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fcbef8f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diffuser = diffuser / np.max(diffuser) * 255\n",
    "cv2.imwrite('output.png', diffuser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "17431b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "path = r\"C:\\rkka_Projects\\physics\\dataset\\diffuser\\test\\phase\\1 (110).jpg\"\n",
    "image = Image.open(path).convert('L')\n",
    "image = np.array(image)\n",
    "image = cv2.resize(image, dsize=(N,N))\n",
    "phase = image / 255 * 2 * np.pi\n",
    "\n",
    "phase = torch.from_numpy(phase).cuda()\n",
    "phase = phase.type(torch.float32)\n",
    "phase = phase.unsqueeze(0).unsqueeze(0)\n",
    "output = model(phase)\n",
    "\n",
    "output = output.squeeze(0).squeeze(0)\n",
    "output = output.detach().cpu().numpy()\n",
    "output = output / np.max(output) * 255\n",
    "Image.fromarray(output.astype('uint8')).save('output.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "b3e8ff8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open('apple.png').convert('L')\n",
    "image = np.array(image)\n",
    "image = cv2.resize(image, dsize=(224,224))\n",
    "phase = image / 255 * 2 * np.pi\n",
    "\n",
    "speckle = create_speckle(phase)\n",
    "speckle = speckle / np.max(speckle) * 225\n",
    "Image.fromarray(speckle.astype('uint8')).save(f'speckle.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca1aa5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7372"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.sim_size2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877a913d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# =========================================================================================\n",
    "# ======================== 수정된 IterativeInverter 모델 ====================================\n",
    "# =========================================================================================\n",
    "\n",
    "class IterativeInverter(nn.Module):\n",
    "    def __init__(self, forward_model: SpeckleSimulator):\n",
    "        super().__init__()\n",
    "        # 1. forward_model에서 학습된 고정 파라미터를 가져와 buffer로 등록합니다.\n",
    "        #    이 파라미터들은 더 이상 학습되지 않습니다.\n",
    "        self.N = forward_model.N\n",
    "        self.sim_size = forward_model.sim_size            # 4096\n",
    "        self.pad_size = forward_model.pad_size            # 1936\n",
    "        self.sim_size2 = forward_model.sim_size2          # 7372\n",
    "        self.pad_size2 = forward_model.pad_size2          # 1638\n",
    "        self.out_size = 2048  # SpeckleSimulator의 crop 크기\n",
    "\n",
    "        # 디바이스 일관성을 위해 forward_model의 파라미터로부터 디바이스 정보를 가져옵니다.\n",
    "        self.device = forward_model.curvature.device\n",
    "        \n",
    "        # 학습된 파라미터들을 buffer로 등록\n",
    "        self.register_buffer('curvature', forward_model.curvature.detach())\n",
    "        self.register_buffer('diffuser', forward_model.diffuser.detach())\n",
    "        self.register_buffer('diffuser_amplitude', forward_model.diffuser_amplitude.detach())\n",
    "        \n",
    "        # 2. Diffuser의 진폭과 위상을 포함하는 시스템 함수 H를 미리 계산합니다.\n",
    "        H = self.diffuser_amplitude * torch.exp(1j * self.diffuser)\n",
    "        self.register_buffer('H', H)\n",
    "        \n",
    "        # 3. H의 역연산에 사용할 켤레 복소수(complex conjugate)를 계산합니다.\n",
    "        epsilon = 1e-8\n",
    "        self.register_buffer('H_inv', torch.conj(self.H) / (torch.abs(self.H)**2 + epsilon))\n",
    "\n",
    "    def forward_propagation(self, input_phase):\n",
    "        \"\"\" 입력 위상 -> 출력 복소수 필드 (SpeckleSimulator의 과정을 정확히 재현) \"\"\"\n",
    "        H = self.diffuser_amplitude * torch.exp(1j * self.diffuser) # Set diffuser as learnable parameter\n",
    "        # H = self.diffuser_net()  # Define diffuser \n",
    "        before_quad = torch.exp(1j * self.curvature)\n",
    "        M = torch.exp(1j * input_phase) * before_quad         # Define field after SLM\n",
    "        M = F.pad(M, pad=(self.pad_size, self.pad_size, self.pad_size, self.pad_size), \n",
    "                               mode='constant', value=0)     # Zero-Pad the field after SLM\n",
    "        \n",
    "        \n",
    "        f = torch.fft.fftshift(torch.fft.fft2(torch.fft.ifftshift(M))) * H # Fourier transform -> multiply diffuser\n",
    "        f = F.pad(f, pad=(self.pad_size2, self.pad_size2, self.pad_size2, self.pad_size2), \n",
    "                               mode='constant', value=0)     # Zero-Pad the field after SLM\n",
    "        U = torch.fft.fftshift(torch.fft.ifft2(torch.fft.ifftshift(f)))         # Inverse Fourier Transform\n",
    "        \n",
    "        out_size = 2048\n",
    "        start = (self.sim_size2 - out_size) // 2\n",
    "        \n",
    "        output_field = U[..., start:start+out_size, start:start+out_size]\n",
    "        \n",
    "        return output_field\n",
    "\n",
    "    def backward_propagation(self, output_field):\n",
    "        \"\"\" 출력 필드 -> 입력 필드 (Forward Propagation의 모든 과정을 정확히 역순으로 수행) \"\"\"\n",
    "        pad_size = (self.sim_size2-2048)//2\n",
    "        U = F.pad(output_field, pad=(pad_size, pad_size, pad_size, pad_size), \n",
    "                               mode='constant', value=0)     # Zero-Pad the field after SLM\n",
    "        \n",
    "        U = torch.fft.fftshift(torch.fft.fft2(torch.fft.ifftshift(U)))\n",
    "        U_real = U.real\n",
    "        U_imag = U.imag\n",
    "        \n",
    "        print(U_real.shape)\n",
    "        \n",
    "        U_real = F.interpolate(U_real, (2048, 2048), mode='linear')\n",
    "        U_imag = F.interpolate(U_imag, (2048, 2048), mode='linear')\n",
    "        f = torch.complex(U_real, U_imag)\n",
    "        f = f * torch.conj(self.H)\n",
    "        M = torch.fft.fftshift(torch.fft.ifft2(torch.fft.ifftshift(f)))\n",
    "        \n",
    "        M = F.interpolate(M, (self.N, self.N), mode='linear')\n",
    "        before_quad = torch.exp(1j * self.curvature)\n",
    "        M = M * torch.conj(before_quad)\n",
    "\n",
    "        return M\n",
    "\n",
    "    def find_phase(self, target_image, iterations=50):\n",
    "        \"\"\" Gerchberg-Saxton 알고리즘으로 목표 이미지를 만드는 최적의 위상을 찾습니다 \"\"\"\n",
    "        # 목표 진폭 계산 (0으로 나누는 것을 방지)\n",
    "        target_amplitude = torch.sqrt(target_image.clamp(min=1e-8))   # (600, 600)\n",
    "        \n",
    "        # 시작점: 입력 위상을 랜덤하게 초기화\n",
    "        input_phase = (torch.rand(600, 600) * 2 * torch.pi).to(self.device)   # (600, 600)\n",
    "\n",
    "        for _ in range(iterations):\n",
    "            # 1. [정방향] 현재 위상으로 출력 필드 계산\n",
    "            output_field = self.forward_propagation(input_phase)   # (2048, 2048)\n",
    "            \n",
    "            # 2. [출력 평면 제약] 계산된 위상은 유지하되, 진폭은 목표 진폭으로 강제 교체\n",
    "            estimated_phase_at_output = torch.angle(output_field)   # (2048, 2048)\n",
    "            corrected_output_field = target_amplitude * torch.exp(1j * estimated_phase_at_output)\n",
    "            \n",
    "            # 3. [역방향] 수정된 출력 필드를 역전파하여 입력 필드 추정\n",
    "            estimated_input_field = self.backward_propagation(corrected_output_field)\n",
    "            \n",
    "            # 4. [입력 평면 제약] SLM은 위상만 조절 가능하므로, 추정된 입력 필드의 위상만 취함\n",
    "            #    ★★ 중요 ★★: 추정된 필드는 (입력 위상 + 곡률)이므로, 곡률을 빼주어야 순수 입력 위상을 얻을 수 있습니다.\n",
    "            input_phase = torch.angle(estimated_input_field) - self.curvature\n",
    "\n",
    "        return input_phase.detach() # 최종적으로 찾아낸 위상을 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f85716f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7372"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.sim_size2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c4d4fec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최적의 위상 패턴을 찾는 중... (반복 최적화)\n",
      "torch.Size([7372, 7372])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input and output must have the same number of spatial dimensions, but got input with spatial dimensions of [] and output size of (2048, 2048). Please provide input tensor in (N, C, d1, d2, ...,dK) format and output size in (o1, o2, ...,oK) format.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m최적의 위상 패턴을 찾는 중... (반복 최적화)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# .find_phase() 메서드를 호출하면 내부적으로 루프가 돌면서 위상을 찾아줍니다.\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m optimized_phase \u001b[38;5;241m=\u001b[39m inverter\u001b[38;5;241m.\u001b[39mfind_phase(target_image, iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m완료!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# 4. 결과 검증: 찾은 위상을 원래의 forward_model에 넣어 사과가 나오는지 확인\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[51], line 100\u001b[0m, in \u001b[0;36mIterativeInverter.find_phase\u001b[1;34m(self, target_image, iterations)\u001b[0m\n\u001b[0;32m     97\u001b[0m corrected_output_field \u001b[38;5;241m=\u001b[39m target_amplitude \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m1\u001b[39mj \u001b[38;5;241m*\u001b[39m estimated_phase_at_output)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# 3. [역방향] 수정된 출력 필드를 역전파하여 입력 필드 추정\u001b[39;00m\n\u001b[1;32m--> 100\u001b[0m estimated_input_field \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackward_propagation(corrected_output_field)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# 4. [입력 평면 제약] SLM은 위상만 조절 가능하므로, 추정된 입력 필드의 위상만 취함\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m#    ★★ 중요 ★★: 추정된 필드는 (입력 위상 + 곡률)이므로, 곡률을 빼주어야 순수 입력 위상을 얻을 수 있습니다.\u001b[39;00m\n\u001b[0;32m    104\u001b[0m input_phase \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mangle(estimated_input_field) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurvature\n",
      "Cell \u001b[1;32mIn[51], line 71\u001b[0m, in \u001b[0;36mIterativeInverter.backward_propagation\u001b[1;34m(self, output_field)\u001b[0m\n\u001b[0;32m     69\u001b[0m U_imag \u001b[38;5;241m=\u001b[39m U\u001b[38;5;241m.\u001b[39mimag\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(U_real\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 71\u001b[0m U_real \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(U_real, (\u001b[38;5;241m2048\u001b[39m, \u001b[38;5;241m2048\u001b[39m), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     72\u001b[0m U_imag \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(U_imag, (\u001b[38;5;241m2048\u001b[39m, \u001b[38;5;241m2048\u001b[39m), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     73\u001b[0m f \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcomplex(U_real, U_imag)\n",
      "File \u001b[1;32mc:\\miniconda3\\envs\\holo\\Lib\\site-packages\\torch\\nn\\functional.py:4566\u001b[0m, in \u001b[0;36minterpolate\u001b[1;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[0;32m   4564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(size, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m   4565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m!=\u001b[39m dim:\n\u001b[1;32m-> 4566\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   4567\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput and output must have the same number of spatial dimensions, but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4568\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput with spatial dimensions of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and output size of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4569\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease provide input tensor in (N, C, d1, d2, ...,dK) format and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4570\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput size in (o1, o2, ...,oK) format.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4571\u001b[0m         )\n\u001b[0;32m   4572\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n\u001b[0;32m   4573\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(_is_integer(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m size):\n",
      "\u001b[1;31mValueError\u001b[0m: Input and output must have the same number of spatial dimensions, but got input with spatial dimensions of [] and output size of (2048, 2048). Please provide input tensor in (N, C, d1, d2, ...,dK) format and output size in (o1, o2, ...,oK) format."
     ]
    }
   ],
   "source": [
    "# 학습된 모델을 'forward_model'로 저장\n",
    "forward_model = model.eval()\n",
    "\n",
    "# 1. 학습된 forward_model로 Inverter 인스턴스 생성\n",
    "inverter = IterativeInverter(forward_model).to(device)\n",
    "\n",
    "# 2. 목표 이미지 준비 (이전과 동일)\n",
    "target_img = Image.open('square.jpg').convert('L')\n",
    "target_img = np.array(target_img)\n",
    "target_ima = 255 - target_img\n",
    "target_img = cv2.resize(target_img, dsize=(2048, 2048))\n",
    "target_img = target_img / np.max(target_img) * 2 * np.pi\n",
    "target_image = torch.from_numpy(target_img).to(device, dtype=torch.float32)\n",
    "\n",
    "# 3. Inverter를 사용하여 최적 위상 찾기\n",
    "print(\"최적의 위상 패턴을 찾는 중... (반복 최적화)\")\n",
    "# .find_phase() 메서드를 호출하면 내부적으로 루프가 돌면서 위상을 찾아줍니다.\n",
    "optimized_phase = inverter.find_phase(target_image, iterations=200)\n",
    "print(\"완료!\")\n",
    "\n",
    "# 4. 결과 검증: 찾은 위상을 원래의 forward_model에 넣어 사과가 나오는지 확인\n",
    "with torch.no_grad():\n",
    "    # 찾은 위상을 정방향 시뮬레이터에 입력\n",
    "    final_output = forward_model(optimized_phase.unsqueeze(0).unsqueeze(0))\n",
    "    final_output = final_output.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f2d3e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = optimized_phase.detach().cpu().numpy()\n",
    "temp = temp / np.max(temp) * 255\n",
    "Image.fromarray(temp.astype('uint8')).save('phase.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "641c5321",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = final_output.detach().cpu().numpy()\n",
    "temp = temp / np.max(temp) * 255\n",
    "Image.fromarray(temp.astype('uint8')).save('result.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "holo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
