{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d514908",
   "metadata": {},
   "source": [
    "<h1> Neural Net </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97e40d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "--- 초기화 완료 ---\n",
      "총 1개의 이미지로 데이터셋 구성 완료.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from lucam import Lucam\n",
    "\n",
    "# --- 기본 설정 (이전과 동일) ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "N = 600\n",
    "total_cam_roi = (450, -500, 450, -500) # Top Bottom Left Right\n",
    "first_order_cam_roi = (500, -450, 450, -500) # Top Bottom Left Right\n",
    "\n",
    "LEARNING_RATE_MODEL = 1e-3\n",
    "LEARNING_RATE_PHASE = 1e-2\n",
    "\n",
    "# --- 🧠 중간 깊이의 뉴럴 네트워크 모델 정의 ---\n",
    "class MediumUNetPropagation(nn.Module):\n",
    "    def __init__(self, in_channels=2, out_channels=1):\n",
    "        super(MediumUNetPropagation, self).__init__()\n",
    "\n",
    "        # --- 인코더 (Contracting Path) ---\n",
    "        # Level 1\n",
    "        self.enc1 = self.conv_block(in_channels, 64)\n",
    "        # Level 2\n",
    "        self.enc2 = self.conv_block(64, 128)\n",
    "        # Level 3 (추가된 깊이)\n",
    "        self.enc3 = self.conv_block(128, 256)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # --- 병목 구간 (Bottleneck) ---\n",
    "        self.bottleneck = self.conv_block(256, 512)\n",
    "\n",
    "        # --- 디코더 (Expanding Path) ---\n",
    "        # Level 3\n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.dec3 = self.conv_block(256 + 256, 256) # Skip connection 포함\n",
    "        # Level 2\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.dec2 = self.conv_block(128 + 128, 128) # Skip connection 포함\n",
    "        # Level 1\n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dec1 = self.conv_block(64 + 64, 64)   # Skip connection 포함\n",
    "\n",
    "        # --- 최종 출력 레이어 ---\n",
    "        self.out_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "    def conv_block(self, in_c, out_c):\n",
    "        \"\"\"두 개의 3x3 Conv와 ReLU, BatchNorm으로 구성된 기본 블록\"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_c, out_c, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_c, out_c, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, phase_map):\n",
    "        # ‼️ 입력 변환: φ -> [cos(φ), sin(φ)]\n",
    "        if phase_map.dim() == 3: # (B, H, W) -> (B, 1, H, W)\n",
    "            phase_map = phase_map.unsqueeze(1)\n",
    "\n",
    "        x_cos = torch.cos(phase_map)\n",
    "        x_sin = torch.sin(phase_map)\n",
    "        x = torch.cat([x_cos, x_sin], dim=1) # (B, 2, N, N)\n",
    "\n",
    "        # --- 인코더 ---\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool(e1))\n",
    "        e3 = self.enc3(self.pool(e2))\n",
    "\n",
    "        # --- 병목 ---\n",
    "        b = self.bottleneck(self.pool(e3))\n",
    "\n",
    "        # --- 디코더 + Skip Connections ---\n",
    "        d3 = self.upconv3(b)\n",
    "        d3 = torch.cat([d3, e3], dim=1)\n",
    "        d3 = self.dec3(d3)\n",
    "\n",
    "        d2 = self.upconv2(d3)\n",
    "        d2 = torch.cat([d2, e2], dim=1)\n",
    "        d2 = self.dec2(d2)\n",
    "\n",
    "        d1 = self.upconv1(d2)\n",
    "        d1 = torch.cat([d1, e1], dim=1)\n",
    "        d1 = self.dec1(d1)\n",
    "\n",
    "        # --- 출력 ---\n",
    "        out = self.out_conv(d1)\n",
    "        return out.squeeze(1) # (B, N, N)\n",
    "\n",
    "# --- 헬퍼 함수 정의 (이전과 동일) ---\n",
    "def save_phase_as_image(phase_tensor, filename):\n",
    "    phase_normalized = (phase_tensor.detach() + torch.pi) / (2 * torch.pi)\n",
    "    phase_uint8 = (phase_normalized * 255).byte().cpu().numpy()\n",
    "    Image.fromarray(phase_uint8).save(filename)\n",
    "\n",
    "def load_and_preprocess_image(path, size=(N, N)):\n",
    "    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None: raise FileNotFoundError(f\"'{path}' 파일을 찾을 수 없습니다.\")\n",
    "    img = cv2.resize(img, dsize=size)\n",
    "    img_float = img.astype(np.float32) / np.max(img) # 0~1 사이로 정규화\n",
    "    return torch.from_numpy(img_float).to('cpu')\n",
    "\n",
    "# --- 🌟 데이터셋 클래스 정의 🌟 ---\n",
    "class HolographyDataset(Dataset):\n",
    "    def __init__(self, image_dir):\n",
    "        # 이미지 파일 경로 리스트 가져오기\n",
    "        self.image_paths = glob.glob(os.path.join(image_dir, '*.jpg')) + \\\n",
    "                           glob.glob(os.path.join(image_dir, '*.png'))\n",
    "                           \n",
    "        lam = 0.532e-6\n",
    "        dx = 12.5e-6\n",
    "        z = 100e-3\n",
    "\n",
    "        lam = torch.tensor(lam).cuda()\n",
    "        dx = torch.tensor(dx).cuda()\n",
    "        z = torch.tensor(z).cuda()\n",
    "\n",
    "        # 각 이미지에 대한 위상 텐서를 저장할 딕셔너리\n",
    "        self.phase_tensors = {}\n",
    "        for path in self.image_paths:\n",
    "            # 초기 위상은 랜덤으로 생성\n",
    "            phase = (torch.pi * torch.ones(N,N)).requires_grad_(True)\n",
    "            self.phase_tensors[path] = phase\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path = self.image_paths[idx]\n",
    "        target_intensity = load_and_preprocess_image(path)\n",
    "        target_intensity = target_intensity / torch.max(target_intensity).item()\n",
    "        target_amplitude = torch.sqrt(target_intensity)\n",
    "        phase_tensor = self.phase_tensors[path]\n",
    "        return target_amplitude.to('cuda'), phase_tensor.to('cuda'), path # 경로도 함께 반환하여 추적\n",
    "\n",
    "# --- 변수 및 모델 초기화 ---\n",
    "model = MediumUNetPropagation().to(device)\n",
    "\n",
    "# 🌟 데이터셋 및 데이터로더 생성\n",
    "# 'images' 폴더에 학습용 이미지를 넣어주세요.\n",
    "dataset = HolographyDataset(image_dir='./circle')\n",
    "# 미니배치 크기. GPU 메모리에 따라 조절.\n",
    "BATCH_SIZE = 1\n",
    "data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# s1, s2 스케일 팩터. 이제 이미지마다 필요할 수 있으나, 우선은 공유\n",
    "s1 = torch.tensor(1.0, device=device, requires_grad=True)\n",
    "s2 = torch.tensor(1.0, device=device, requires_grad=True)\n",
    "\n",
    "# ‼️ 옵티마이저 정의. 이제 위상 텐서는 데이터셋 안에 있으므로, 별도로 최적화\n",
    "optimizer_model = optim.Adam(list(model.parameters()) + [s2], lr=LEARNING_RATE_MODEL)\n",
    "# 위상 텐서들을 모아서 phase 옵티마이저에 등록\n",
    "all_phase_params = list(dataset.phase_tensors.values()) + [s1]\n",
    "optimizer_phase = optim.Adam(all_phase_params, lr=LEARNING_RATE_PHASE)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "print(f\"\\n--- 초기화 완료 ---\")\n",
    "print(f\"총 {len(dataset)}개의 이미지로 데이터셋 구성 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08ecedfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function Lucam.__del__ at 0x0000023636CB1E40>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\holo\\envs\\holo\\Lib\\site-packages\\lucam\\lucam.py\", line 877, in __del__\n",
      "    assert not self._displaying_window\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\holo\\envs\\holo\\Lib\\site-packages\\lucam\\lucam.py\", line 957, in __getattr__\n",
      "    raise AttributeError(f\"'Lucam' object has no attribute '{name}'\")\n",
      "AttributeError: 'Lucam' object has no attribute '_displaying_window'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Epoch 1/100 ====================\n",
      "Epoch 1, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 1, Batch 1 [2/2] 모델 업데이트 완료. Loss: 1.009946\n",
      "\n",
      "==================== Epoch 2/100 ====================\n",
      "Epoch 2, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 2, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.972435\n",
      "\n",
      "==================== Epoch 3/100 ====================\n",
      "Epoch 3, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 3, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.970831\n",
      "\n",
      "==================== Epoch 4/100 ====================\n",
      "Epoch 4, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 4, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.968198\n",
      "\n",
      "==================== Epoch 5/100 ====================\n",
      "Epoch 5, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 5, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.967725\n",
      "\n",
      "==================== Epoch 6/100 ====================\n",
      "Epoch 6, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 6, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.967306\n",
      "\n",
      "==================== Epoch 7/100 ====================\n",
      "Epoch 7, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 7, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.966722\n",
      "\n",
      "==================== Epoch 8/100 ====================\n",
      "Epoch 8, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 8, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.966284\n",
      "\n",
      "==================== Epoch 9/100 ====================\n",
      "Epoch 9, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 9, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.965825\n",
      "\n",
      "==================== Epoch 10/100 ====================\n",
      "Epoch 10, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 10, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.965488\n",
      "\n",
      "==================== Epoch 11/100 ====================\n",
      "Epoch 11, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 11, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.964478\n",
      "\n",
      "==================== Epoch 12/100 ====================\n",
      "Epoch 12, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 12, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.963807\n",
      "\n",
      "==================== Epoch 13/100 ====================\n",
      "Epoch 13, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 13, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.962833\n",
      "\n",
      "==================== Epoch 14/100 ====================\n",
      "Epoch 14, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 14, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.961938\n",
      "\n",
      "==================== Epoch 15/100 ====================\n",
      "Epoch 15, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 15, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.962201\n",
      "\n",
      "==================== Epoch 16/100 ====================\n",
      "Epoch 16, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 16, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.962587\n",
      "\n",
      "==================== Epoch 17/100 ====================\n",
      "Epoch 17, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 17, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.959309\n",
      "\n",
      "==================== Epoch 18/100 ====================\n",
      "Epoch 18, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 18, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.960071\n",
      "\n",
      "==================== Epoch 19/100 ====================\n",
      "Epoch 19, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 19, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.958285\n",
      "\n",
      "==================== Epoch 20/100 ====================\n",
      "Epoch 20, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 20, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.958315\n",
      "\n",
      "==================== Epoch 21/100 ====================\n",
      "Epoch 21, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 21, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.956820\n",
      "\n",
      "==================== Epoch 22/100 ====================\n",
      "Epoch 22, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 22, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.956593\n",
      "\n",
      "==================== Epoch 23/100 ====================\n",
      "Epoch 23, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 23, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.955179\n",
      "\n",
      "==================== Epoch 24/100 ====================\n",
      "Epoch 24, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 24, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.954967\n",
      "\n",
      "==================== Epoch 25/100 ====================\n",
      "Epoch 25, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 25, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.954343\n",
      "\n",
      "==================== Epoch 26/100 ====================\n",
      "Epoch 26, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 26, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.953692\n",
      "\n",
      "==================== Epoch 27/100 ====================\n",
      "Epoch 27, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 27, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.953278\n",
      "\n",
      "==================== Epoch 28/100 ====================\n",
      "Epoch 28, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 28, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.953326\n",
      "\n",
      "==================== Epoch 29/100 ====================\n",
      "Epoch 29, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 29, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.951469\n",
      "\n",
      "==================== Epoch 30/100 ====================\n",
      "Epoch 30, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 30, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.951598\n",
      "\n",
      "==================== Epoch 31/100 ====================\n",
      "Epoch 31, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 31, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.951908\n",
      "\n",
      "==================== Epoch 32/100 ====================\n",
      "Epoch 32, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 32, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.951325\n",
      "\n",
      "==================== Epoch 33/100 ====================\n",
      "Epoch 33, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 33, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.949513\n",
      "\n",
      "==================== Epoch 34/100 ====================\n",
      "Epoch 34, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 34, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.949031\n",
      "\n",
      "==================== Epoch 35/100 ====================\n",
      "Epoch 35, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 35, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.947692\n",
      "\n",
      "==================== Epoch 36/100 ====================\n",
      "Epoch 36, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 36, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.947739\n",
      "\n",
      "==================== Epoch 37/100 ====================\n",
      "Epoch 37, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 37, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.946799\n",
      "\n",
      "==================== Epoch 38/100 ====================\n",
      "Epoch 38, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 38, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.946331\n",
      "\n",
      "==================== Epoch 39/100 ====================\n",
      "Epoch 39, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 39, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.945960\n",
      "\n",
      "==================== Epoch 40/100 ====================\n",
      "Epoch 40, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 40, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.944954\n",
      "\n",
      "==================== Epoch 41/100 ====================\n",
      "Epoch 41, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 41, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.944215\n",
      "\n",
      "==================== Epoch 42/100 ====================\n",
      "Epoch 42, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 42, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.943160\n",
      "\n",
      "==================== Epoch 43/100 ====================\n",
      "Epoch 43, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 43, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.942162\n",
      "\n",
      "==================== Epoch 44/100 ====================\n",
      "Epoch 44, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 44, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.941352\n",
      "\n",
      "==================== Epoch 45/100 ====================\n",
      "Epoch 45, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 45, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.940455\n",
      "\n",
      "==================== Epoch 46/100 ====================\n",
      "Epoch 46, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 46, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.938935\n",
      "\n",
      "==================== Epoch 47/100 ====================\n",
      "Epoch 47, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 47, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.938211\n",
      "\n",
      "==================== Epoch 48/100 ====================\n",
      "Epoch 48, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 48, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.937331\n",
      "\n",
      "==================== Epoch 49/100 ====================\n",
      "Epoch 49, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 49, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.940119\n",
      "\n",
      "==================== Epoch 50/100 ====================\n",
      "Epoch 50, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 50, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.937799\n",
      "\n",
      "==================== Epoch 51/100 ====================\n",
      "Epoch 51, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 51, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.938295\n",
      "\n",
      "==================== Epoch 52/100 ====================\n",
      "Epoch 52, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 52, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.938639\n",
      "\n",
      "==================== Epoch 53/100 ====================\n",
      "Epoch 53, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 53, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.936236\n",
      "\n",
      "==================== Epoch 54/100 ====================\n",
      "Epoch 54, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 54, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.934542\n",
      "\n",
      "==================== Epoch 55/100 ====================\n",
      "Epoch 55, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 55, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.933621\n",
      "\n",
      "==================== Epoch 56/100 ====================\n",
      "Epoch 56, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 56, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.932150\n",
      "\n",
      "==================== Epoch 57/100 ====================\n",
      "Epoch 57, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 57, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.931000\n",
      "\n",
      "==================== Epoch 58/100 ====================\n",
      "Epoch 58, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 58, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.929142\n",
      "\n",
      "==================== Epoch 59/100 ====================\n",
      "Epoch 59, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 59, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.927535\n",
      "\n",
      "==================== Epoch 60/100 ====================\n",
      "Epoch 60, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 60, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.928161\n",
      "\n",
      "==================== Epoch 61/100 ====================\n",
      "Epoch 61, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 61, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.930898\n",
      "\n",
      "==================== Epoch 62/100 ====================\n",
      "Epoch 62, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 62, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.929791\n",
      "\n",
      "==================== Epoch 63/100 ====================\n",
      "Epoch 63, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 63, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.928090\n",
      "\n",
      "==================== Epoch 64/100 ====================\n",
      "Epoch 64, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 64, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.924795\n",
      "\n",
      "==================== Epoch 65/100 ====================\n",
      "Epoch 65, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 65, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.924175\n",
      "\n",
      "==================== Epoch 66/100 ====================\n",
      "Epoch 66, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 66, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.924489\n",
      "\n",
      "==================== Epoch 67/100 ====================\n",
      "Epoch 67, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 67, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.922434\n",
      "\n",
      "==================== Epoch 68/100 ====================\n",
      "Epoch 68, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 68, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.920804\n",
      "\n",
      "==================== Epoch 69/100 ====================\n",
      "Epoch 69, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 69, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.917755\n",
      "\n",
      "==================== Epoch 70/100 ====================\n",
      "Epoch 70, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 70, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.916381\n",
      "\n",
      "==================== Epoch 71/100 ====================\n",
      "Epoch 71, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 71, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.914240\n",
      "\n",
      "==================== Epoch 72/100 ====================\n",
      "Epoch 72, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 72, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.912144\n",
      "\n",
      "==================== Epoch 73/100 ====================\n",
      "Epoch 73, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 73, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.909243\n",
      "\n",
      "==================== Epoch 74/100 ====================\n",
      "Epoch 74, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 74, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.910587\n",
      "\n",
      "==================== Epoch 75/100 ====================\n",
      "Epoch 75, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 75, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.918092\n",
      "\n",
      "==================== Epoch 76/100 ====================\n",
      "Epoch 76, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 76, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.915336\n",
      "\n",
      "==================== Epoch 77/100 ====================\n",
      "Epoch 77, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 77, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.914877\n",
      "\n",
      "==================== Epoch 78/100 ====================\n",
      "Epoch 78, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 78, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.909586\n",
      "\n",
      "==================== Epoch 79/100 ====================\n",
      "Epoch 79, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 79, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.908597\n",
      "\n",
      "==================== Epoch 80/100 ====================\n",
      "Epoch 80, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 80, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.904890\n",
      "\n",
      "==================== Epoch 81/100 ====================\n",
      "Epoch 81, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 81, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.903451\n",
      "\n",
      "==================== Epoch 82/100 ====================\n",
      "Epoch 82, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 82, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.899502\n",
      "\n",
      "==================== Epoch 83/100 ====================\n",
      "Epoch 83, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 83, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.896368\n",
      "\n",
      "==================== Epoch 84/100 ====================\n",
      "Epoch 84, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 84, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.896344\n",
      "\n",
      "==================== Epoch 85/100 ====================\n",
      "Epoch 85, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 85, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.894869\n",
      "\n",
      "==================== Epoch 86/100 ====================\n",
      "Epoch 86, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 86, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.892188\n",
      "\n",
      "==================== Epoch 87/100 ====================\n",
      "Epoch 87, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 87, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.887560\n",
      "\n",
      "==================== Epoch 88/100 ====================\n",
      "Epoch 88, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 88, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.885192\n",
      "\n",
      "==================== Epoch 89/100 ====================\n",
      "Epoch 89, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 89, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.882917\n",
      "\n",
      "==================== Epoch 90/100 ====================\n",
      "Epoch 90, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 90, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.884209\n",
      "\n",
      "==================== Epoch 91/100 ====================\n",
      "Epoch 91, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 91, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.880582\n",
      "\n",
      "==================== Epoch 92/100 ====================\n",
      "Epoch 92, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 92, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.876877\n",
      "\n",
      "==================== Epoch 93/100 ====================\n",
      "Epoch 93, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 93, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.878083\n",
      "\n",
      "==================== Epoch 94/100 ====================\n",
      "Epoch 94, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 94, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.878387\n",
      "\n",
      "==================== Epoch 95/100 ====================\n",
      "Epoch 95, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 95, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.873060\n",
      "\n",
      "==================== Epoch 96/100 ====================\n",
      "Epoch 96, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 96, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.872157\n",
      "\n",
      "==================== Epoch 97/100 ====================\n",
      "Epoch 97, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 97, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.864224\n",
      "\n",
      "==================== Epoch 98/100 ====================\n",
      "Epoch 98, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 98, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.863226\n",
      "\n",
      "==================== Epoch 99/100 ====================\n",
      "Epoch 99, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 99, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.862415\n",
      "\n",
      "==================== Epoch 100/100 ====================\n",
      "Epoch 100, Batch 1 [1/2] 위상 업데이트 완료.\n",
      "Epoch 100, Batch 1 [2/2] 모델 업데이트 완료. Loss: 0.860082\n"
     ]
    }
   ],
   "source": [
    "from pytorch_msssim import ssim, ms_ssim # Multi-Scale SSIM이 더 성능이 좋을 수 있습니다.\n",
    "import torch.nn.functional as F\n",
    "\n",
    "NUM_EPOCHS = 100 # 전체 데이터셋을 몇 번 반복할지\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\n{'='*20} Epoch {epoch + 1}/{NUM_EPOCHS} {'='*20}\")\n",
    "    \n",
    "        # data_loader에서 미니배치 단위로 데이터를 가져옴\n",
    "    for i, (target_amplitudes, phase_tensors, image_paths) in enumerate(data_loader):\n",
    "        \n",
    "        # --- 단계 1: 위상 업데이트 -\n",
    "        model.eval()\n",
    "        optimizer_phase.zero_grad()\n",
    "        \n",
    "        # U-Net 모델은 배치 입력을 처리할 수 있도록 수정됨\n",
    "        prediction_for_phase = model(phase_tensors)\n",
    "\n",
    "        loss_phase = loss_fn(s1 * prediction_for_phase, target_amplitudes**2)\n",
    "        loss_phase.backward()\n",
    "        optimizer_phase.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Batch {i+1} [1/2] 위상 업데이트 완료.\")\n",
    "\n",
    "        # --- 단계 2: 모델 업데이트 ---\n",
    "        # 이 단계에서는 미니배치의 각 이미지에 대해 물리적 실험을 반복해야 함\n",
    "        \n",
    "        captured_amplitudes_batch = []\n",
    "        # 배치 내 각 샘플에 대해 SLM 띄우고 촬영\n",
    "        for j in range(len(image_paths)):\n",
    "            phase_to_display = phase_tensors[j]\n",
    "            \n",
    "            save_phase_as_image(phase_to_display, 'test.png')\n",
    "            \n",
    "            slm_process = subprocess.Popen(['python', 'test.py'])\n",
    "            time.sleep(2)\n",
    "            \n",
    "            # ‼️ 실제 카메라 촬영 로직\n",
    "            camera = Lucam()\n",
    "            capture = camera.TakeSnapshot()\n",
    "            # capture = capture[500:-450, 450:-500]\n",
    "            capture = cv2.resize(capture, dsize=(N, N))\n",
    "            cv2.imwrite('captured_image.png', capture)\n",
    "            \n",
    "            slm_process.terminate()\n",
    "            slm_process.wait()\n",
    "\n",
    "            captured_intensity = load_and_preprocess_image('captured_image.png')\n",
    "            captured_intensity = captured_intensity / torch.max(captured_intensity).item()\n",
    "            captured_amp = torch.sqrt(captured_intensity)\n",
    "            captured_amplitudes_batch.append(captured_amp)\n",
    "        \n",
    "        # 촬영된 이미지들을 하나의 배치 텐서로 결합\n",
    "        captured_amplitudes = torch.stack(captured_amplitudes_batch)\n",
    "        \n",
    "        # 모델 학습\n",
    "        model.train()\n",
    "        optimizer_model.zero_grad()\n",
    "        \n",
    "        # phase_tensors는 업데이트되었지만, 모델 학습에는 이전 상태를 사용해야 함\n",
    "        prediction_for_model = model(phase_tensors.detach())\n",
    "\n",
    "        # loss_model = loss_fn(s2 * prediction_for_model, (captured_amplitudes**2).cuda())\n",
    "\n",
    "        loss_ssim = 1 - ssim(prediction_for_model.unsqueeze(0), (captured_amplitudes.unsqueeze(0)**2).cuda(), data_range=1.0, size_average=True)\n",
    "        loss_model = loss_ssim\n",
    "\n",
    "        loss_model.backward()\n",
    "        optimizer_model.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Batch {i+1} [2/2] 모델 업데이트 완료. Loss: {loss_model.item():.6f}\")\n",
    "        temp = phase_tensors.clone()\n",
    "        output = model(temp.detach()).detach().cpu().numpy()[0]\n",
    "        output = (output - np.min(output)) / (np.max(output) - np.min(output))\n",
    "        output = output * 255\n",
    "        Image.fromarray(output.astype('uint8')).save('output.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f55d79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Epoch 1/15 ====================\n",
      "Epoch 1, Batch 1 [1/2] 위상 업데이트 완료. Loss: 0.999463\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 54\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# ... (실제 SLM 구동 및 카메라 촬영 로직) ...\u001b[39;00m\n\u001b[32m     53\u001b[39m slm_process = subprocess.Popen([\u001b[33m'\u001b[39m\u001b[33mpython\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtest.py\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m camera = Lucam()\n\u001b[32m     56\u001b[39m capture = camera.TakeSnapshot() \n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ... (이전 코드는 대부분 동일) ...\n",
    "from pytorch_msssim import ssim\n",
    "\n",
    "# 손실 함수는 MSE가 더 적합할 수 있습니다. 스페클 패턴의 픽셀 단위 비교이므로.\n",
    "# loss_fn = torch.nn.MSELoss() \n",
    "# 푸리에 도메인 비교에는 SSIM도 효과적일 수 있습니다. 선택적으로 사용해보세요.\n",
    "loss_fn_fourier = lambda pred, target: 1 - ssim(pred.unsqueeze(1), target.unsqueeze(1), data_range=1.0, size_average=True)\n",
    "loss_fn_model = torch.nn.MSELoss()\n",
    "\n",
    "NUM_EPOCHS = 15\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\n{'='*20} Epoch {epoch + 1}/{NUM_EPOCHS} {'='*20}\")\n",
    "\n",
    "    for i, (target_amplitudes, phase_tensors, image_paths) in enumerate(data_loader):\n",
    "        # target_amplitudes는 이제 스페클이 아닌 '최종 목표 이미지'의 진폭입니다.\n",
    "        # 이 코드에서는 제곱된 intensity를 사용하겠습니다.\n",
    "        target_image = target_amplitudes**2 \n",
    "\n",
    "        # --- 단계 1: 위상 업데이트 ---\n",
    "        model.eval() # 모델은 평가 모드로 두고, 위상만 최적화\n",
    "        optimizer_phase.zero_grad()\n",
    "\n",
    "        # 모델을 통해 예상 스페클 패턴을 얻음\n",
    "        predicted_speckle = model(phase_tensors)\n",
    "\n",
    "        # ‼️ 핵심: 예상 스페클을 푸리에 변환\n",
    "        # 푸리에 변환 및 파워 스펙트럼 계산 (intensity of Fourier transform)\n",
    "        predicted_fft = torch.fft.fftshift(torch.fft.fft2(predicted_speckle), dim=(-2, -1))\n",
    "        predicted_power_spectrum = torch.abs(predicted_fft)**2\n",
    "        \n",
    "        # 정규화 (매우 중요)\n",
    "        # 배치 내 각 이미지에 대해 독립적으로 정규화\n",
    "        for b in range(predicted_power_spectrum.shape[0]):\n",
    "             predicted_power_spectrum[b] /= torch.max(predicted_power_spectrum[b]).item()\n",
    "\n",
    "        # 푸리에 변환된 결과와 목표 이미지를 비교\n",
    "        loss_phase = loss_fn_fourier(predicted_power_spectrum, target_image) # 또는 MSE 사용\n",
    "        loss_phase.backward()\n",
    "        optimizer_phase.step() # 위상 텐서만 업데이트\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Batch {i+1} [1/2] 위상 업데이트 완료. Loss: {loss_phase.item():.6f}\")\n",
    "\n",
    "        # --- 단계 2: 모델 업데이트 ---\n",
    "        # 1단계에서 업데이트된 위상을 실제 시스템에 적용하고 모델을 교정\n",
    "        \n",
    "        captured_intensities_batch = []\n",
    "        for j in range(len(image_paths)):\n",
    "            phase_to_display = phase_tensors[j]\n",
    "            save_phase_as_image(phase_to_display, 'test.png') # 디버깅용\n",
    "\n",
    "            # ... (실제 SLM 구동 및 카메라 촬영 로직) ...\n",
    "            slm_process = subprocess.Popen(['python', 'test.py'])\n",
    "            time.sleep(2)\n",
    "            camera = Lucam()\n",
    "            capture = camera.TakeSnapshot() \n",
    "            capture = cv2.resize(capture, dsize=(N, N))\n",
    "            cv2.imwrite('captured_image.png', capture)\n",
    "\n",
    "            slm_process.terminate()\n",
    "            slm_process.wait()\n",
    "\n",
    "            # 촬영된 실제 스페클 패턴 로드\n",
    "            captured_intensity = load_and_preprocess_image('captured_image.png').to(device)\n",
    "            captured_intensity = captured_intensity / torch.max(captured_intensity).item()\n",
    "            captured_intensities_batch.append(captured_intensity)\n",
    "\n",
    "        captured_intensities = torch.stack(captured_intensities_batch)\n",
    "\n",
    "        # 모델 학습 모드로 전환\n",
    "        model.train()\n",
    "        optimizer_model.zero_grad()\n",
    "\n",
    "        # 모델이 예측한 스페클과 실제 촬영된 스페클을 비교\n",
    "        # phase_tensors는 위상 업데이트 후의 상태이며, 모델 입력 시 detach()하여\n",
    "        # 이 손실이 위상에 역전파되지 않도록 함\n",
    "        prediction_for_model = model(phase_tensors.detach())\n",
    "\n",
    "        # 스케일 팩터(s2)를 포함하여 손실 계산\n",
    "        loss_model = loss_fn_model(s2 * prediction_for_model, captured_intensities)\n",
    "        loss_model.backward()\n",
    "        optimizer_model.step() # 모델과 s2만 업데이트\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Batch {i+1} [2/2] 모델 업데이트 완료. Loss: {loss_model.item():.6f}\")\n",
    "\n",
    "        # --- 시각화 및 디버깅 (매우 중요) ---\n",
    "        # 현재 예측된 푸리에 스펙트럼이 어떻게 생겼는지 확인\n",
    "        final_recon = predicted_power_spectrum.detach().cpu().numpy()[0]\n",
    "        Image.fromarray((final_recon * 255).astype('uint8')).save(f'output.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a380691f",
   "metadata": {},
   "outputs": [],
   "source": [
    "speckle = Image.open('captured_image.png').convert('L')\n",
    "speckle = np.array(speckle)\n",
    "autocorr = np.fft.fftshift(np.fft.fft2(speckle))\n",
    "power = np.abs(autocorr)**2\n",
    "power = power / np.max(power) * 255\n",
    "Image.fromarray(power.astype('uint8')).save('output.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "holo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
