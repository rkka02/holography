{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d514908",
   "metadata": {},
   "source": [
    "<h1> Neural Net </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97e40d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "--- ì´ˆê¸°í™” ì™„ë£Œ ---\n",
      "ì´ 1ê°œì˜ ì´ë¯¸ì§€ë¡œ ë°ì´í„°ì…‹ êµ¬ì„± ì™„ë£Œ.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from lucam import Lucam\n",
    "\n",
    "# --- ê¸°ë³¸ ì„¤ì • (ì´ì „ê³¼ ë™ì¼) ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "N = 600\n",
    "total_cam_roi = (450, -500, 450, -500) # Top Bottom Left Right\n",
    "first_order_cam_roi = (500, -450, 450, -500) # Top Bottom Left Right\n",
    "\n",
    "LEARNING_RATE_MODEL = 1e-3\n",
    "LEARNING_RATE_PHASE = 1e-2\n",
    "\n",
    "# --- ğŸ§  ì¤‘ê°„ ê¹Šì´ì˜ ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬ ëª¨ë¸ ì •ì˜ ---\n",
    "class MediumUNetPropagation(nn.Module):\n",
    "    def __init__(self, in_channels=2, out_channels=1):\n",
    "        super(MediumUNetPropagation, self).__init__()\n",
    "\n",
    "        # --- ì¸ì½”ë” (Contracting Path) ---\n",
    "        # Level 1\n",
    "        self.enc1 = self.conv_block(in_channels, 64)\n",
    "        # Level 2\n",
    "        self.enc2 = self.conv_block(64, 128)\n",
    "        # Level 3 (ì¶”ê°€ëœ ê¹Šì´)\n",
    "        self.enc3 = self.conv_block(128, 256)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # --- ë³‘ëª© êµ¬ê°„ (Bottleneck) ---\n",
    "        self.bottleneck = self.conv_block(256, 512)\n",
    "\n",
    "        # --- ë””ì½”ë” (Expanding Path) ---\n",
    "        # Level 3\n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.dec3 = self.conv_block(256 + 256, 256) # Skip connection í¬í•¨\n",
    "        # Level 2\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.dec2 = self.conv_block(128 + 128, 128) # Skip connection í¬í•¨\n",
    "        # Level 1\n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dec1 = self.conv_block(64 + 64, 64)   # Skip connection í¬í•¨\n",
    "\n",
    "        # --- ìµœì¢… ì¶œë ¥ ë ˆì´ì–´ ---\n",
    "        self.out_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "    def conv_block(self, in_c, out_c):\n",
    "        \"\"\"ë‘ ê°œì˜ 3x3 Convì™€ ReLU, BatchNormìœ¼ë¡œ êµ¬ì„±ëœ ê¸°ë³¸ ë¸”ë¡\"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_c, out_c, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_c, out_c, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, phase_map):\n",
    "        # â€¼ï¸ ì…ë ¥ ë³€í™˜: Ï† -> [cos(Ï†), sin(Ï†)]\n",
    "        if phase_map.dim() == 3: # (B, H, W) -> (B, 1, H, W)\n",
    "            phase_map = phase_map.unsqueeze(1)\n",
    "\n",
    "        x_cos = torch.cos(phase_map)\n",
    "        x_sin = torch.sin(phase_map)\n",
    "        x = torch.cat([x_cos, x_sin], dim=1) # (B, 2, N, N)\n",
    "\n",
    "        # --- ì¸ì½”ë” ---\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool(e1))\n",
    "        e3 = self.enc3(self.pool(e2))\n",
    "\n",
    "        # --- ë³‘ëª© ---\n",
    "        b = self.bottleneck(self.pool(e3))\n",
    "\n",
    "        # --- ë””ì½”ë” + Skip Connections ---\n",
    "        d3 = self.upconv3(b)\n",
    "        d3 = torch.cat([d3, e3], dim=1)\n",
    "        d3 = self.dec3(d3)\n",
    "\n",
    "        d2 = self.upconv2(d3)\n",
    "        d2 = torch.cat([d2, e2], dim=1)\n",
    "        d2 = self.dec2(d2)\n",
    "\n",
    "        d1 = self.upconv1(d2)\n",
    "        d1 = torch.cat([d1, e1], dim=1)\n",
    "        d1 = self.dec1(d1)\n",
    "\n",
    "        # --- ì¶œë ¥ ---\n",
    "        out = self.out_conv(d1)\n",
    "        return out.squeeze(1) # (B, N, N)\n",
    "\n",
    "# --- í—¬í¼ í•¨ìˆ˜ ì •ì˜ (ì´ì „ê³¼ ë™ì¼) ---\n",
    "def save_phase_as_image(phase_tensor, filename):\n",
    "    phase_normalized = (phase_tensor.detach() + torch.pi) / (2 * torch.pi)\n",
    "    phase_uint8 = (phase_normalized * 255).byte().cpu().numpy()\n",
    "    Image.fromarray(phase_uint8).save(filename)\n",
    "\n",
    "def load_and_preprocess_image(path, size=(N, N)):\n",
    "    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None: raise FileNotFoundError(f\"'{path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    img = cv2.resize(img, dsize=size)\n",
    "    img_float = img.astype(np.float32) / np.max(img) # 0~1 ì‚¬ì´ë¡œ ì •ê·œí™”\n",
    "    return torch.from_numpy(img_float).to('cpu')\n",
    "\n",
    "# --- ğŸŒŸ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜ ğŸŒŸ ---\n",
    "class HolographyDataset(Dataset):\n",
    "    def __init__(self, image_dir):\n",
    "        # ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°\n",
    "        self.image_paths = glob.glob(os.path.join(image_dir, '*.jpg')) + \\\n",
    "                           glob.glob(os.path.join(image_dir, '*.png'))\n",
    "                           \n",
    "        lam = 0.532e-6\n",
    "        dx = 12.5e-6\n",
    "        z = 100e-3\n",
    "\n",
    "        lam = torch.tensor(lam).cuda()\n",
    "        dx = torch.tensor(dx).cuda()\n",
    "        z = torch.tensor(z).cuda()\n",
    "\n",
    "        # ê° ì´ë¯¸ì§€ì— ëŒ€í•œ ìœ„ìƒ í…ì„œë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬\n",
    "        self.phase_tensors = {}\n",
    "        for path in self.image_paths:\n",
    "            # ì´ˆê¸° ìœ„ìƒì€ ëœë¤ìœ¼ë¡œ ìƒì„±\n",
    "            phase = (torch.pi * torch.ones(N,N)).requires_grad_(True)\n",
    "            self.phase_tensors[path] = phase\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path = self.image_paths[idx]\n",
    "        target_intensity = load_and_preprocess_image(path)\n",
    "        target_intensity = target_intensity / torch.max(target_intensity).item()\n",
    "        target_amplitude = torch.sqrt(target_intensity)\n",
    "        phase_tensor = self.phase_tensors[path]\n",
    "        return target_amplitude.to('cuda'), phase_tensor.to('cuda'), path # ê²½ë¡œë„ í•¨ê»˜ ë°˜í™˜í•˜ì—¬ ì¶”ì \n",
    "\n",
    "# --- ë³€ìˆ˜ ë° ëª¨ë¸ ì´ˆê¸°í™” ---\n",
    "model = MediumUNetPropagation().to(device)\n",
    "\n",
    "# ğŸŒŸ ë°ì´í„°ì…‹ ë° ë°ì´í„°ë¡œë” ìƒì„±\n",
    "# 'images' í´ë”ì— í•™ìŠµìš© ì´ë¯¸ì§€ë¥¼ ë„£ì–´ì£¼ì„¸ìš”.\n",
    "dataset = HolographyDataset(image_dir='./circle')\n",
    "# ë¯¸ë‹ˆë°°ì¹˜ í¬ê¸°. GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì ˆ.\n",
    "BATCH_SIZE = 1\n",
    "data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# s1, s2 ìŠ¤ì¼€ì¼ íŒ©í„°. ì´ì œ ì´ë¯¸ì§€ë§ˆë‹¤ í•„ìš”í•  ìˆ˜ ìˆìœ¼ë‚˜, ìš°ì„ ì€ ê³µìœ \n",
    "s1 = torch.tensor(1.0, device=device, requires_grad=True)\n",
    "s2 = torch.tensor(1.0, device=device, requires_grad=True)\n",
    "\n",
    "# â€¼ï¸ ì˜µí‹°ë§ˆì´ì € ì •ì˜. ì´ì œ ìœ„ìƒ í…ì„œëŠ” ë°ì´í„°ì…‹ ì•ˆì— ìˆìœ¼ë¯€ë¡œ, ë³„ë„ë¡œ ìµœì í™”\n",
    "optimizer_model = optim.Adam(list(model.parameters()) + [s2], lr=LEARNING_RATE_MODEL)\n",
    "# ìœ„ìƒ í…ì„œë“¤ì„ ëª¨ì•„ì„œ phase ì˜µí‹°ë§ˆì´ì €ì— ë“±ë¡\n",
    "all_phase_params = list(dataset.phase_tensors.values()) + [s1]\n",
    "optimizer_phase = optim.Adam(all_phase_params, lr=LEARNING_RATE_PHASE)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "print(f\"\\n--- ì´ˆê¸°í™” ì™„ë£Œ ---\")\n",
    "print(f\"ì´ {len(dataset)}ê°œì˜ ì´ë¯¸ì§€ë¡œ ë°ì´í„°ì…‹ êµ¬ì„± ì™„ë£Œ.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08ecedfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function Lucam.__del__ at 0x0000023636CB1E40>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\holo\\envs\\holo\\Lib\\site-packages\\lucam\\lucam.py\", line 877, in __del__\n",
      "    assert not self._displaying_window\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\holo\\envs\\holo\\Lib\\site-packages\\lucam\\lucam.py\", line 957, in __getattr__\n",
      "    raise AttributeError(f\"'Lucam' object has no attribute '{name}'\")\n",
      "AttributeError: 'Lucam' object has no attribute '_displaying_window'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Epoch 1/100 ====================\n",
      "Epoch 1, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 1, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 1.009946\n",
      "\n",
      "==================== Epoch 2/100 ====================\n",
      "Epoch 2, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 2, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.972435\n",
      "\n",
      "==================== Epoch 3/100 ====================\n",
      "Epoch 3, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 3, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.970831\n",
      "\n",
      "==================== Epoch 4/100 ====================\n",
      "Epoch 4, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 4, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.968198\n",
      "\n",
      "==================== Epoch 5/100 ====================\n",
      "Epoch 5, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 5, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.967725\n",
      "\n",
      "==================== Epoch 6/100 ====================\n",
      "Epoch 6, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 6, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.967306\n",
      "\n",
      "==================== Epoch 7/100 ====================\n",
      "Epoch 7, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 7, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.966722\n",
      "\n",
      "==================== Epoch 8/100 ====================\n",
      "Epoch 8, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 8, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.966284\n",
      "\n",
      "==================== Epoch 9/100 ====================\n",
      "Epoch 9, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 9, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.965825\n",
      "\n",
      "==================== Epoch 10/100 ====================\n",
      "Epoch 10, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 10, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.965488\n",
      "\n",
      "==================== Epoch 11/100 ====================\n",
      "Epoch 11, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 11, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.964478\n",
      "\n",
      "==================== Epoch 12/100 ====================\n",
      "Epoch 12, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 12, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.963807\n",
      "\n",
      "==================== Epoch 13/100 ====================\n",
      "Epoch 13, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 13, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.962833\n",
      "\n",
      "==================== Epoch 14/100 ====================\n",
      "Epoch 14, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 14, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.961938\n",
      "\n",
      "==================== Epoch 15/100 ====================\n",
      "Epoch 15, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 15, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.962201\n",
      "\n",
      "==================== Epoch 16/100 ====================\n",
      "Epoch 16, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 16, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.962587\n",
      "\n",
      "==================== Epoch 17/100 ====================\n",
      "Epoch 17, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 17, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.959309\n",
      "\n",
      "==================== Epoch 18/100 ====================\n",
      "Epoch 18, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 18, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.960071\n",
      "\n",
      "==================== Epoch 19/100 ====================\n",
      "Epoch 19, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 19, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.958285\n",
      "\n",
      "==================== Epoch 20/100 ====================\n",
      "Epoch 20, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 20, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.958315\n",
      "\n",
      "==================== Epoch 21/100 ====================\n",
      "Epoch 21, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 21, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.956820\n",
      "\n",
      "==================== Epoch 22/100 ====================\n",
      "Epoch 22, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 22, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.956593\n",
      "\n",
      "==================== Epoch 23/100 ====================\n",
      "Epoch 23, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 23, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.955179\n",
      "\n",
      "==================== Epoch 24/100 ====================\n",
      "Epoch 24, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 24, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.954967\n",
      "\n",
      "==================== Epoch 25/100 ====================\n",
      "Epoch 25, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 25, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.954343\n",
      "\n",
      "==================== Epoch 26/100 ====================\n",
      "Epoch 26, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 26, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.953692\n",
      "\n",
      "==================== Epoch 27/100 ====================\n",
      "Epoch 27, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 27, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.953278\n",
      "\n",
      "==================== Epoch 28/100 ====================\n",
      "Epoch 28, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 28, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.953326\n",
      "\n",
      "==================== Epoch 29/100 ====================\n",
      "Epoch 29, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 29, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.951469\n",
      "\n",
      "==================== Epoch 30/100 ====================\n",
      "Epoch 30, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 30, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.951598\n",
      "\n",
      "==================== Epoch 31/100 ====================\n",
      "Epoch 31, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 31, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.951908\n",
      "\n",
      "==================== Epoch 32/100 ====================\n",
      "Epoch 32, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 32, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.951325\n",
      "\n",
      "==================== Epoch 33/100 ====================\n",
      "Epoch 33, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 33, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.949513\n",
      "\n",
      "==================== Epoch 34/100 ====================\n",
      "Epoch 34, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 34, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.949031\n",
      "\n",
      "==================== Epoch 35/100 ====================\n",
      "Epoch 35, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 35, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.947692\n",
      "\n",
      "==================== Epoch 36/100 ====================\n",
      "Epoch 36, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 36, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.947739\n",
      "\n",
      "==================== Epoch 37/100 ====================\n",
      "Epoch 37, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 37, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.946799\n",
      "\n",
      "==================== Epoch 38/100 ====================\n",
      "Epoch 38, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 38, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.946331\n",
      "\n",
      "==================== Epoch 39/100 ====================\n",
      "Epoch 39, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 39, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.945960\n",
      "\n",
      "==================== Epoch 40/100 ====================\n",
      "Epoch 40, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 40, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.944954\n",
      "\n",
      "==================== Epoch 41/100 ====================\n",
      "Epoch 41, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 41, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.944215\n",
      "\n",
      "==================== Epoch 42/100 ====================\n",
      "Epoch 42, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 42, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.943160\n",
      "\n",
      "==================== Epoch 43/100 ====================\n",
      "Epoch 43, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 43, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.942162\n",
      "\n",
      "==================== Epoch 44/100 ====================\n",
      "Epoch 44, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 44, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.941352\n",
      "\n",
      "==================== Epoch 45/100 ====================\n",
      "Epoch 45, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 45, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.940455\n",
      "\n",
      "==================== Epoch 46/100 ====================\n",
      "Epoch 46, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 46, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.938935\n",
      "\n",
      "==================== Epoch 47/100 ====================\n",
      "Epoch 47, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 47, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.938211\n",
      "\n",
      "==================== Epoch 48/100 ====================\n",
      "Epoch 48, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 48, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.937331\n",
      "\n",
      "==================== Epoch 49/100 ====================\n",
      "Epoch 49, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 49, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.940119\n",
      "\n",
      "==================== Epoch 50/100 ====================\n",
      "Epoch 50, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 50, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.937799\n",
      "\n",
      "==================== Epoch 51/100 ====================\n",
      "Epoch 51, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 51, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.938295\n",
      "\n",
      "==================== Epoch 52/100 ====================\n",
      "Epoch 52, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 52, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.938639\n",
      "\n",
      "==================== Epoch 53/100 ====================\n",
      "Epoch 53, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 53, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.936236\n",
      "\n",
      "==================== Epoch 54/100 ====================\n",
      "Epoch 54, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 54, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.934542\n",
      "\n",
      "==================== Epoch 55/100 ====================\n",
      "Epoch 55, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 55, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.933621\n",
      "\n",
      "==================== Epoch 56/100 ====================\n",
      "Epoch 56, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 56, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.932150\n",
      "\n",
      "==================== Epoch 57/100 ====================\n",
      "Epoch 57, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 57, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.931000\n",
      "\n",
      "==================== Epoch 58/100 ====================\n",
      "Epoch 58, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 58, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.929142\n",
      "\n",
      "==================== Epoch 59/100 ====================\n",
      "Epoch 59, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 59, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.927535\n",
      "\n",
      "==================== Epoch 60/100 ====================\n",
      "Epoch 60, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 60, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.928161\n",
      "\n",
      "==================== Epoch 61/100 ====================\n",
      "Epoch 61, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 61, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.930898\n",
      "\n",
      "==================== Epoch 62/100 ====================\n",
      "Epoch 62, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 62, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.929791\n",
      "\n",
      "==================== Epoch 63/100 ====================\n",
      "Epoch 63, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 63, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.928090\n",
      "\n",
      "==================== Epoch 64/100 ====================\n",
      "Epoch 64, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 64, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.924795\n",
      "\n",
      "==================== Epoch 65/100 ====================\n",
      "Epoch 65, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 65, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.924175\n",
      "\n",
      "==================== Epoch 66/100 ====================\n",
      "Epoch 66, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 66, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.924489\n",
      "\n",
      "==================== Epoch 67/100 ====================\n",
      "Epoch 67, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 67, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.922434\n",
      "\n",
      "==================== Epoch 68/100 ====================\n",
      "Epoch 68, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 68, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.920804\n",
      "\n",
      "==================== Epoch 69/100 ====================\n",
      "Epoch 69, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 69, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.917755\n",
      "\n",
      "==================== Epoch 70/100 ====================\n",
      "Epoch 70, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 70, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.916381\n",
      "\n",
      "==================== Epoch 71/100 ====================\n",
      "Epoch 71, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 71, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.914240\n",
      "\n",
      "==================== Epoch 72/100 ====================\n",
      "Epoch 72, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 72, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.912144\n",
      "\n",
      "==================== Epoch 73/100 ====================\n",
      "Epoch 73, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 73, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.909243\n",
      "\n",
      "==================== Epoch 74/100 ====================\n",
      "Epoch 74, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 74, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.910587\n",
      "\n",
      "==================== Epoch 75/100 ====================\n",
      "Epoch 75, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 75, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.918092\n",
      "\n",
      "==================== Epoch 76/100 ====================\n",
      "Epoch 76, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 76, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.915336\n",
      "\n",
      "==================== Epoch 77/100 ====================\n",
      "Epoch 77, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 77, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.914877\n",
      "\n",
      "==================== Epoch 78/100 ====================\n",
      "Epoch 78, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 78, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.909586\n",
      "\n",
      "==================== Epoch 79/100 ====================\n",
      "Epoch 79, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 79, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.908597\n",
      "\n",
      "==================== Epoch 80/100 ====================\n",
      "Epoch 80, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 80, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.904890\n",
      "\n",
      "==================== Epoch 81/100 ====================\n",
      "Epoch 81, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 81, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.903451\n",
      "\n",
      "==================== Epoch 82/100 ====================\n",
      "Epoch 82, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 82, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.899502\n",
      "\n",
      "==================== Epoch 83/100 ====================\n",
      "Epoch 83, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 83, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.896368\n",
      "\n",
      "==================== Epoch 84/100 ====================\n",
      "Epoch 84, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 84, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.896344\n",
      "\n",
      "==================== Epoch 85/100 ====================\n",
      "Epoch 85, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 85, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.894869\n",
      "\n",
      "==================== Epoch 86/100 ====================\n",
      "Epoch 86, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 86, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.892188\n",
      "\n",
      "==================== Epoch 87/100 ====================\n",
      "Epoch 87, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 87, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.887560\n",
      "\n",
      "==================== Epoch 88/100 ====================\n",
      "Epoch 88, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 88, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.885192\n",
      "\n",
      "==================== Epoch 89/100 ====================\n",
      "Epoch 89, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 89, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.882917\n",
      "\n",
      "==================== Epoch 90/100 ====================\n",
      "Epoch 90, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 90, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.884209\n",
      "\n",
      "==================== Epoch 91/100 ====================\n",
      "Epoch 91, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 91, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.880582\n",
      "\n",
      "==================== Epoch 92/100 ====================\n",
      "Epoch 92, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 92, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.876877\n",
      "\n",
      "==================== Epoch 93/100 ====================\n",
      "Epoch 93, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 93, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.878083\n",
      "\n",
      "==================== Epoch 94/100 ====================\n",
      "Epoch 94, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 94, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.878387\n",
      "\n",
      "==================== Epoch 95/100 ====================\n",
      "Epoch 95, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 95, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.873060\n",
      "\n",
      "==================== Epoch 96/100 ====================\n",
      "Epoch 96, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 96, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.872157\n",
      "\n",
      "==================== Epoch 97/100 ====================\n",
      "Epoch 97, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 97, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.864224\n",
      "\n",
      "==================== Epoch 98/100 ====================\n",
      "Epoch 98, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 98, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.863226\n",
      "\n",
      "==================== Epoch 99/100 ====================\n",
      "Epoch 99, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 99, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.862415\n",
      "\n",
      "==================== Epoch 100/100 ====================\n",
      "Epoch 100, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\n",
      "Epoch 100, Batch 1 [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.860082\n"
     ]
    }
   ],
   "source": [
    "from pytorch_msssim import ssim, ms_ssim # Multi-Scale SSIMì´ ë” ì„±ëŠ¥ì´ ì¢‹ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "import torch.nn.functional as F\n",
    "\n",
    "NUM_EPOCHS = 100 # ì „ì²´ ë°ì´í„°ì…‹ì„ ëª‡ ë²ˆ ë°˜ë³µí• ì§€\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\n{'='*20} Epoch {epoch + 1}/{NUM_EPOCHS} {'='*20}\")\n",
    "    \n",
    "        # data_loaderì—ì„œ ë¯¸ë‹ˆë°°ì¹˜ ë‹¨ìœ„ë¡œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´\n",
    "    for i, (target_amplitudes, phase_tensors, image_paths) in enumerate(data_loader):\n",
    "        \n",
    "        # --- ë‹¨ê³„ 1: ìœ„ìƒ ì—…ë°ì´íŠ¸ -\n",
    "        model.eval()\n",
    "        optimizer_phase.zero_grad()\n",
    "        \n",
    "        # U-Net ëª¨ë¸ì€ ë°°ì¹˜ ì…ë ¥ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ ìˆ˜ì •ë¨\n",
    "        prediction_for_phase = model(phase_tensors)\n",
    "\n",
    "        loss_phase = loss_fn(s1 * prediction_for_phase, target_amplitudes**2)\n",
    "        loss_phase.backward()\n",
    "        optimizer_phase.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Batch {i+1} [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ.\")\n",
    "\n",
    "        # --- ë‹¨ê³„ 2: ëª¨ë¸ ì—…ë°ì´íŠ¸ ---\n",
    "        # ì´ ë‹¨ê³„ì—ì„œëŠ” ë¯¸ë‹ˆë°°ì¹˜ì˜ ê° ì´ë¯¸ì§€ì— ëŒ€í•´ ë¬¼ë¦¬ì  ì‹¤í—˜ì„ ë°˜ë³µí•´ì•¼ í•¨\n",
    "        \n",
    "        captured_amplitudes_batch = []\n",
    "        # ë°°ì¹˜ ë‚´ ê° ìƒ˜í”Œì— ëŒ€í•´ SLM ë„ìš°ê³  ì´¬ì˜\n",
    "        for j in range(len(image_paths)):\n",
    "            phase_to_display = phase_tensors[j]\n",
    "            \n",
    "            save_phase_as_image(phase_to_display, 'test.png')\n",
    "            \n",
    "            slm_process = subprocess.Popen(['python', 'test.py'])\n",
    "            time.sleep(2)\n",
    "            \n",
    "            # â€¼ï¸ ì‹¤ì œ ì¹´ë©”ë¼ ì´¬ì˜ ë¡œì§\n",
    "            camera = Lucam()\n",
    "            capture = camera.TakeSnapshot()\n",
    "            # capture = capture[500:-450, 450:-500]\n",
    "            capture = cv2.resize(capture, dsize=(N, N))\n",
    "            cv2.imwrite('captured_image.png', capture)\n",
    "            \n",
    "            slm_process.terminate()\n",
    "            slm_process.wait()\n",
    "\n",
    "            captured_intensity = load_and_preprocess_image('captured_image.png')\n",
    "            captured_intensity = captured_intensity / torch.max(captured_intensity).item()\n",
    "            captured_amp = torch.sqrt(captured_intensity)\n",
    "            captured_amplitudes_batch.append(captured_amp)\n",
    "        \n",
    "        # ì´¬ì˜ëœ ì´ë¯¸ì§€ë“¤ì„ í•˜ë‚˜ì˜ ë°°ì¹˜ í…ì„œë¡œ ê²°í•©\n",
    "        captured_amplitudes = torch.stack(captured_amplitudes_batch)\n",
    "        \n",
    "        # ëª¨ë¸ í•™ìŠµ\n",
    "        model.train()\n",
    "        optimizer_model.zero_grad()\n",
    "        \n",
    "        # phase_tensorsëŠ” ì—…ë°ì´íŠ¸ë˜ì—ˆì§€ë§Œ, ëª¨ë¸ í•™ìŠµì—ëŠ” ì´ì „ ìƒíƒœë¥¼ ì‚¬ìš©í•´ì•¼ í•¨\n",
    "        prediction_for_model = model(phase_tensors.detach())\n",
    "\n",
    "        # loss_model = loss_fn(s2 * prediction_for_model, (captured_amplitudes**2).cuda())\n",
    "\n",
    "        loss_ssim = 1 - ssim(prediction_for_model.unsqueeze(0), (captured_amplitudes.unsqueeze(0)**2).cuda(), data_range=1.0, size_average=True)\n",
    "        loss_model = loss_ssim\n",
    "\n",
    "        loss_model.backward()\n",
    "        optimizer_model.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Batch {i+1} [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: {loss_model.item():.6f}\")\n",
    "        temp = phase_tensors.clone()\n",
    "        output = model(temp.detach()).detach().cpu().numpy()[0]\n",
    "        output = (output - np.min(output)) / (np.max(output) - np.min(output))\n",
    "        output = output * 255\n",
    "        Image.fromarray(output.astype('uint8')).save('output.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f55d79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Epoch 1/15 ====================\n",
      "Epoch 1, Batch 1 [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: 0.999463\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 54\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# ... (ì‹¤ì œ SLM êµ¬ë™ ë° ì¹´ë©”ë¼ ì´¬ì˜ ë¡œì§) ...\u001b[39;00m\n\u001b[32m     53\u001b[39m slm_process = subprocess.Popen([\u001b[33m'\u001b[39m\u001b[33mpython\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtest.py\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m camera = Lucam()\n\u001b[32m     56\u001b[39m capture = camera.TakeSnapshot() \n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ... (ì´ì „ ì½”ë“œëŠ” ëŒ€ë¶€ë¶„ ë™ì¼) ...\n",
    "from pytorch_msssim import ssim\n",
    "\n",
    "# ì†ì‹¤ í•¨ìˆ˜ëŠ” MSEê°€ ë” ì í•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìŠ¤í˜í´ íŒ¨í„´ì˜ í”½ì…€ ë‹¨ìœ„ ë¹„êµì´ë¯€ë¡œ.\n",
    "# loss_fn = torch.nn.MSELoss() \n",
    "# í‘¸ë¦¬ì— ë„ë©”ì¸ ë¹„êµì—ëŠ” SSIMë„ íš¨ê³¼ì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì„ íƒì ìœ¼ë¡œ ì‚¬ìš©í•´ë³´ì„¸ìš”.\n",
    "loss_fn_fourier = lambda pred, target: 1 - ssim(pred.unsqueeze(1), target.unsqueeze(1), data_range=1.0, size_average=True)\n",
    "loss_fn_model = torch.nn.MSELoss()\n",
    "\n",
    "NUM_EPOCHS = 15\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\n{'='*20} Epoch {epoch + 1}/{NUM_EPOCHS} {'='*20}\")\n",
    "\n",
    "    for i, (target_amplitudes, phase_tensors, image_paths) in enumerate(data_loader):\n",
    "        # target_amplitudesëŠ” ì´ì œ ìŠ¤í˜í´ì´ ì•„ë‹Œ 'ìµœì¢… ëª©í‘œ ì´ë¯¸ì§€'ì˜ ì§„í­ì…ë‹ˆë‹¤.\n",
    "        # ì´ ì½”ë“œì—ì„œëŠ” ì œê³±ëœ intensityë¥¼ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤.\n",
    "        target_image = target_amplitudes**2 \n",
    "\n",
    "        # --- ë‹¨ê³„ 1: ìœ„ìƒ ì—…ë°ì´íŠ¸ ---\n",
    "        model.eval() # ëª¨ë¸ì€ í‰ê°€ ëª¨ë“œë¡œ ë‘ê³ , ìœ„ìƒë§Œ ìµœì í™”\n",
    "        optimizer_phase.zero_grad()\n",
    "\n",
    "        # ëª¨ë¸ì„ í†µí•´ ì˜ˆìƒ ìŠ¤í˜í´ íŒ¨í„´ì„ ì–»ìŒ\n",
    "        predicted_speckle = model(phase_tensors)\n",
    "\n",
    "        # â€¼ï¸ í•µì‹¬: ì˜ˆìƒ ìŠ¤í˜í´ì„ í‘¸ë¦¬ì— ë³€í™˜\n",
    "        # í‘¸ë¦¬ì— ë³€í™˜ ë° íŒŒì›Œ ìŠ¤í™íŠ¸ëŸ¼ ê³„ì‚° (intensity of Fourier transform)\n",
    "        predicted_fft = torch.fft.fftshift(torch.fft.fft2(predicted_speckle), dim=(-2, -1))\n",
    "        predicted_power_spectrum = torch.abs(predicted_fft)**2\n",
    "        \n",
    "        # ì •ê·œí™” (ë§¤ìš° ì¤‘ìš”)\n",
    "        # ë°°ì¹˜ ë‚´ ê° ì´ë¯¸ì§€ì— ëŒ€í•´ ë…ë¦½ì ìœ¼ë¡œ ì •ê·œí™”\n",
    "        for b in range(predicted_power_spectrum.shape[0]):\n",
    "             predicted_power_spectrum[b] /= torch.max(predicted_power_spectrum[b]).item()\n",
    "\n",
    "        # í‘¸ë¦¬ì— ë³€í™˜ëœ ê²°ê³¼ì™€ ëª©í‘œ ì´ë¯¸ì§€ë¥¼ ë¹„êµ\n",
    "        loss_phase = loss_fn_fourier(predicted_power_spectrum, target_image) # ë˜ëŠ” MSE ì‚¬ìš©\n",
    "        loss_phase.backward()\n",
    "        optimizer_phase.step() # ìœ„ìƒ í…ì„œë§Œ ì—…ë°ì´íŠ¸\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Batch {i+1} [1/2] ìœ„ìƒ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: {loss_phase.item():.6f}\")\n",
    "\n",
    "        # --- ë‹¨ê³„ 2: ëª¨ë¸ ì—…ë°ì´íŠ¸ ---\n",
    "        # 1ë‹¨ê³„ì—ì„œ ì—…ë°ì´íŠ¸ëœ ìœ„ìƒì„ ì‹¤ì œ ì‹œìŠ¤í…œì— ì ìš©í•˜ê³  ëª¨ë¸ì„ êµì •\n",
    "        \n",
    "        captured_intensities_batch = []\n",
    "        for j in range(len(image_paths)):\n",
    "            phase_to_display = phase_tensors[j]\n",
    "            save_phase_as_image(phase_to_display, 'test.png') # ë””ë²„ê¹…ìš©\n",
    "\n",
    "            # ... (ì‹¤ì œ SLM êµ¬ë™ ë° ì¹´ë©”ë¼ ì´¬ì˜ ë¡œì§) ...\n",
    "            slm_process = subprocess.Popen(['python', 'test.py'])\n",
    "            time.sleep(2)\n",
    "            camera = Lucam()\n",
    "            capture = camera.TakeSnapshot() \n",
    "            capture = cv2.resize(capture, dsize=(N, N))\n",
    "            cv2.imwrite('captured_image.png', capture)\n",
    "\n",
    "            slm_process.terminate()\n",
    "            slm_process.wait()\n",
    "\n",
    "            # ì´¬ì˜ëœ ì‹¤ì œ ìŠ¤í˜í´ íŒ¨í„´ ë¡œë“œ\n",
    "            captured_intensity = load_and_preprocess_image('captured_image.png').to(device)\n",
    "            captured_intensity = captured_intensity / torch.max(captured_intensity).item()\n",
    "            captured_intensities_batch.append(captured_intensity)\n",
    "\n",
    "        captured_intensities = torch.stack(captured_intensities_batch)\n",
    "\n",
    "        # ëª¨ë¸ í•™ìŠµ ëª¨ë“œë¡œ ì „í™˜\n",
    "        model.train()\n",
    "        optimizer_model.zero_grad()\n",
    "\n",
    "        # ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ìŠ¤í˜í´ê³¼ ì‹¤ì œ ì´¬ì˜ëœ ìŠ¤í˜í´ì„ ë¹„êµ\n",
    "        # phase_tensorsëŠ” ìœ„ìƒ ì—…ë°ì´íŠ¸ í›„ì˜ ìƒíƒœì´ë©°, ëª¨ë¸ ì…ë ¥ ì‹œ detach()í•˜ì—¬\n",
    "        # ì´ ì†ì‹¤ì´ ìœ„ìƒì— ì—­ì „íŒŒë˜ì§€ ì•Šë„ë¡ í•¨\n",
    "        prediction_for_model = model(phase_tensors.detach())\n",
    "\n",
    "        # ìŠ¤ì¼€ì¼ íŒ©í„°(s2)ë¥¼ í¬í•¨í•˜ì—¬ ì†ì‹¤ ê³„ì‚°\n",
    "        loss_model = loss_fn_model(s2 * prediction_for_model, captured_intensities)\n",
    "        loss_model.backward()\n",
    "        optimizer_model.step() # ëª¨ë¸ê³¼ s2ë§Œ ì—…ë°ì´íŠ¸\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Batch {i+1} [2/2] ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ. Loss: {loss_model.item():.6f}\")\n",
    "\n",
    "        # --- ì‹œê°í™” ë° ë””ë²„ê¹… (ë§¤ìš° ì¤‘ìš”) ---\n",
    "        # í˜„ì¬ ì˜ˆì¸¡ëœ í‘¸ë¦¬ì— ìŠ¤í™íŠ¸ëŸ¼ì´ ì–´ë–»ê²Œ ìƒê²¼ëŠ”ì§€ í™•ì¸\n",
    "        final_recon = predicted_power_spectrum.detach().cpu().numpy()[0]\n",
    "        Image.fromarray((final_recon * 255).astype('uint8')).save(f'output.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a380691f",
   "metadata": {},
   "outputs": [],
   "source": [
    "speckle = Image.open('captured_image.png').convert('L')\n",
    "speckle = np.array(speckle)\n",
    "autocorr = np.fft.fftshift(np.fft.fft2(speckle))\n",
    "power = np.abs(autocorr)**2\n",
    "power = power / np.max(power) * 255\n",
    "Image.fromarray(power.astype('uint8')).save('output.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "holo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
